{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from utilities import f1_m, recall_m, precision_m\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH = 128\n",
    "FRAME_GAP = 11\n",
    "NUM_FEATURES = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "\n",
    "    #Create CNN model\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    # # compile the model\n",
    "    # optimizer = keras.optimizers.SGD(learning_rate=0.0000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # model.compile(\n",
    "    #     optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    # )\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"output_dim\": self.output_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# Subclassed layer\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = 4\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None), name=\"input\")\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "\n",
    "    x = layers.Dense(units=embed_dim, activation='gelu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "    # optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # # compile the model\n",
    "    # model.compile(\n",
    "    #     optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    # )\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_early_fusion_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = 4\n",
    "\n",
    "    # Create Transformer-based model\n",
    "    inputs_rgb = keras.Input(shape=(None, None), name=\"input_image\")\n",
    "    x1 = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs_rgb)\n",
    "    x1 = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x1)\n",
    "    x1 = layers.Dense(units=embed_dim, activation='gelu')(x1)\n",
    "    x1 = layers.LayerNormalization()(x1)\n",
    "    x1 = layers.GlobalMaxPooling1D()(x1)\n",
    "    x1 = layers.Dropout(0.5)(x1)\n",
    "\n",
    "    #Create CNN model\n",
    "    inputs_spec = keras.Input(shape=(IMG_SIZE,IMG_SIZE,3), name=\"input_spectrogram\")\n",
    "    # # x2 = keras.Sequential()(inputs_spec)\n",
    "    # x2 = layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3))(inputs_spec)\n",
    "    # x2 = layers.Conv2D(64, (3, 3), activation='relu')(x2)\n",
    "    # x2 = layers.MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "    # x2 = layers.Dropout(0.25)(x2)\n",
    "    # x2 = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x2)\n",
    "    # x2 = layers.Conv2D(64, (3, 3), activation='relu')(x2)\n",
    "    # x2 = layers.MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "    # x2 = layers.Dropout(0.5)(x2)\n",
    "    # x2 = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x2)\n",
    "    # x2 = layers.Conv2D(128, (3, 3), activation='relu')(x2)\n",
    "    # x2 = layers.MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "    # x2 = layers.Dropout(0.5)(x2)\n",
    "    # x2 = layers.Flatten()(x2)\n",
    "    # x2 = layers.Dense(512, activation='relu')(x2)\n",
    "    # x2 = layers.Dropout(0.5)(x2)\n",
    "\n",
    "    #audio_temp/\n",
    "    model_pretrained = VGG19(include_top=True)#, weights=\"imagenet\")\n",
    "    x2 = model_pretrained(inputs_spec)\n",
    "    x2 = layers.Dense(4096, activation='relu', name='predictions1', dtype='float32')(x2)\n",
    "\n",
    "\n",
    "    # EARLY FUSION\n",
    "    x = layers.concatenate([x1, x2])\n",
    "    # x = keras.Sequential()(x)\n",
    "    x = layers.Dense(4096, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    \n",
    "    model = keras.Model(inputs=[inputs_rgb, inputs_spec], outputs=outputs) # Inputs go into two different layers\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # optimizer = keras.optimizers.SGD(learning_rate=0.0000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_early_fusion_model(model_1,model_2):\n",
    "#     x1 = model_1.output\n",
    "#     x2 = model_2.output\n",
    "#     classes = 4\n",
    "\n",
    "#     # LATE FUSION\n",
    "#     x = layers.concatenate([x1, x2])\n",
    "#     x = keras.Sequential()(x)\n",
    "#     x = layers.Dropout(0.5)(x)\n",
    "#     outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "#     model = keras.Model(inputs=[model_1.input, model_2.input], outputs=outputs) # Inputs go into two different layers\n",
    "\n",
    "#     optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "#     # compile the model\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "#     )\n",
    "    \n",
    "#     model.summary()\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train multimodal video classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_data, train_labels = np.load(\"extracted_data/train_data.npy\"), np.load(\"extracted_data/train_labels.npy\")\n",
    "val_image_data, val_labels = np.load(\"extracted_data/val_data.npy\"), np.load(\"extracted_data/val_labels.npy\")\n",
    "test_image_data, test_labels = np.load(\"extracted_data/test_data.npy\"), np.load(\"extracted_data/test_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrograms = glob.glob('extracted_train_spectrogram/*')\n",
    "val_spectrograms = glob.glob('extracted_val_spectrogram/*')\n",
    "test_spectrograms = glob.glob('extracted_test_spectrogram/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing import image\n",
    "train_audio_data = []\n",
    "val_audio_data = []\n",
    "test_audio_data = []\n",
    "\n",
    "for f in train_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    train_audio_data.append(img)\n",
    "    \n",
    "train_audio_data = np.array(train_audio_data)\n",
    "\n",
    "for f in val_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    val_audio_data.append(img)\n",
    "    \n",
    "val_audio_data = np.array(val_audio_data)\n",
    "\n",
    "for f in test_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    test_audio_data.append(img)\n",
    "    \n",
    "test_audio_data = np.array(test_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    log_dir = \"logs/fit/early_fusion_temp_2\" \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    filepath = os.getcwd() + \"/early_fusion_temp_2/classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, monitor='val_f1_m',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "    with tf.device('/device:CPU:0'):\n",
    "        # model = get_early_fusion_model(transformer,cnn)\n",
    "        model = get_early_fusion_model()\n",
    "        history = model.fit(\n",
    "            [train_image_data, train_audio_data],\n",
    "            train_labels,\n",
    "            validation_data=([val_image_data, val_audio_data],val_labels),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint, tensorboard_callback],\n",
    "        )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    # _, accuracy = model.evaluate(test_data, test_labels)\n",
    "    # evaluate the model\n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate([test_image_data, test_audio_data], test_labels, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "    print(f\"Precision: {round(precision, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "frame_position_embedding (Posit (None, None, 1024)   131072      input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "transformer_layer (TransformerE (None, None, 1024)   4211716     frame_position_embedding[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 1024)   1049600     transformer_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, None, 1024)   2048        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_spectrogram (InputLayer)  [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 1024)         0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "vgg19 (Functional)              (None, 1000)         143667240   input_spectrogram[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "predictions1 (Dense)            (None, 4096)         4100096     vgg19[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5120)         0           dropout[0][0]                    \n",
      "                                                                 predictions1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4096)         20975616    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            16388       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 174,153,776\n",
      "Trainable params: 174,153,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "30/30 [==============================] - 372s 12s/step - loss: 0.6909 - accuracy: 0.3577 - f1_m: 0.3315 - precision_m: 0.4228 - recall_m: 0.3060 - val_loss: 0.4920 - val_accuracy: 0.3982 - val_f1_m: 0.2171 - val_precision_m: 0.3917 - val_recall_m: 0.1518\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.21709, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp_2\\classifier\n",
      "Epoch 2/30\n",
      "30/30 [==============================] - 371s 12s/step - loss: 0.4823 - accuracy: 0.4299 - f1_m: 0.4001 - precision_m: 0.5083 - recall_m: 0.3435 - val_loss: 0.4545 - val_accuracy: 0.4513 - val_f1_m: 0.3408 - val_precision_m: 0.5556 - val_recall_m: 0.2489\n",
      "\n",
      "Epoch 00002: val_f1_m improved from 0.21709 to 0.34081, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp_2\\classifier\n",
      "Epoch 3/30\n",
      "30/30 [==============================] - 374s 12s/step - loss: 0.4658 - accuracy: 0.4437 - f1_m: 0.4308 - precision_m: 0.5400 - recall_m: 0.3765 - val_loss: 0.4691 - val_accuracy: 0.3894 - val_f1_m: 0.1769 - val_precision_m: 0.4187 - val_recall_m: 0.1127\n",
      "\n",
      "Epoch 00003: val_f1_m did not improve from 0.34081\n",
      "Epoch 4/30\n",
      "30/30 [==============================] - 375s 12s/step - loss: 0.4300 - accuracy: 0.4586 - f1_m: 0.4642 - precision_m: 0.5852 - recall_m: 0.3961 - val_loss: 0.4388 - val_accuracy: 0.4425 - val_f1_m: 0.4793 - val_precision_m: 0.5859 - val_recall_m: 0.4185\n",
      "\n",
      "Epoch 00004: val_f1_m improved from 0.34081 to 0.47931, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp_2\\classifier\n",
      "Epoch 5/30\n",
      "30/30 [==============================] - 375s 12s/step - loss: 0.4089 - accuracy: 0.4820 - f1_m: 0.5054 - precision_m: 0.6219 - recall_m: 0.4367 - val_loss: 0.4360 - val_accuracy: 0.4248 - val_f1_m: 0.3245 - val_precision_m: 0.6609 - val_recall_m: 0.2478\n",
      "\n",
      "Epoch 00005: val_f1_m did not improve from 0.47931\n",
      "Epoch 6/30\n",
      "30/30 [==============================] - 350s 12s/step - loss: 0.3990 - accuracy: 0.4979 - f1_m: 0.5144 - precision_m: 0.6070 - recall_m: 0.4546 - val_loss: 0.4144 - val_accuracy: 0.4248 - val_f1_m: 0.4283 - val_precision_m: 0.5574 - val_recall_m: 0.3873\n",
      "\n",
      "Epoch 00006: val_f1_m did not improve from 0.47931\n",
      "Epoch 7/30\n",
      "30/30 [==============================] - 336s 11s/step - loss: 0.3769 - accuracy: 0.5127 - f1_m: 0.5747 - precision_m: 0.6532 - recall_m: 0.5282 - val_loss: 0.4516 - val_accuracy: 0.4071 - val_f1_m: 0.3902 - val_precision_m: 0.6125 - val_recall_m: 0.3013\n",
      "\n",
      "Epoch 00007: val_f1_m did not improve from 0.47931\n",
      "Epoch 8/30\n",
      "30/30 [==============================] - 337s 11s/step - loss: 0.3617 - accuracy: 0.5223 - f1_m: 0.5638 - precision_m: 0.6608 - recall_m: 0.5017 - val_loss: 0.4149 - val_accuracy: 0.4336 - val_f1_m: 0.5961 - val_precision_m: 0.6727 - val_recall_m: 0.5402\n",
      "\n",
      "Epoch 00008: val_f1_m improved from 0.47931 to 0.59611, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp_2\\classifier\n",
      "Epoch 9/30\n",
      "30/30 [==============================] - 336s 11s/step - loss: 0.3501 - accuracy: 0.5456 - f1_m: 0.5907 - precision_m: 0.6634 - recall_m: 0.5559 - val_loss: 0.4437 - val_accuracy: 0.3894 - val_f1_m: 0.4164 - val_precision_m: 0.6466 - val_recall_m: 0.3214\n",
      "\n",
      "Epoch 00009: val_f1_m did not improve from 0.59611\n",
      "Epoch 10/30\n",
      "30/30 [==============================] - 353s 12s/step - loss: 0.3311 - accuracy: 0.5202 - f1_m: 0.6202 - precision_m: 0.7190 - recall_m: 0.5574 - val_loss: 0.4230 - val_accuracy: 0.4513 - val_f1_m: 0.4863 - val_precision_m: 0.6326 - val_recall_m: 0.4129\n",
      "\n",
      "Epoch 00010: val_f1_m did not improve from 0.59611\n",
      "Epoch 11/30\n",
      "30/30 [==============================] - 377s 13s/step - loss: 0.3199 - accuracy: 0.5446 - f1_m: 0.6470 - precision_m: 0.7406 - recall_m: 0.5929 - val_loss: 0.4357 - val_accuracy: 0.4248 - val_f1_m: 0.5671 - val_precision_m: 0.6283 - val_recall_m: 0.5201\n",
      "\n",
      "Epoch 00011: val_f1_m did not improve from 0.59611\n",
      "Epoch 12/30\n",
      "30/30 [==============================] - 378s 13s/step - loss: 0.2948 - accuracy: 0.5775 - f1_m: 0.6877 - precision_m: 0.7405 - recall_m: 0.6563 - val_loss: 0.4584 - val_accuracy: 0.4159 - val_f1_m: 0.4756 - val_precision_m: 0.6888 - val_recall_m: 0.3795\n",
      "\n",
      "Epoch 00012: val_f1_m did not improve from 0.59611\n",
      "Epoch 13/30\n",
      "30/30 [==============================] - 378s 13s/step - loss: 0.2606 - accuracy: 0.5977 - f1_m: 0.7283 - precision_m: 0.7851 - recall_m: 0.6867 - val_loss: 0.4914 - val_accuracy: 0.4336 - val_f1_m: 0.5549 - val_precision_m: 0.6867 - val_recall_m: 0.4676\n",
      "\n",
      "Epoch 00013: val_f1_m did not improve from 0.59611\n",
      "Epoch 14/30\n",
      "30/30 [==============================] - 377s 13s/step - loss: 0.2472 - accuracy: 0.5955 - f1_m: 0.7256 - precision_m: 0.7776 - recall_m: 0.6885 - val_loss: 0.4681 - val_accuracy: 0.4513 - val_f1_m: 0.5392 - val_precision_m: 0.6698 - val_recall_m: 0.4565\n",
      "\n",
      "Epoch 00014: val_f1_m did not improve from 0.59611\n",
      "Epoch 15/30\n",
      "30/30 [==============================] - 354s 12s/step - loss: 0.2179 - accuracy: 0.6168 - f1_m: 0.7733 - precision_m: 0.8203 - recall_m: 0.7381 - val_loss: 0.5192 - val_accuracy: 0.4248 - val_f1_m: 0.5191 - val_precision_m: 0.6444 - val_recall_m: 0.4431\n",
      "\n",
      "Epoch 00015: val_f1_m did not improve from 0.59611\n",
      "Epoch 16/30\n",
      "30/30 [==============================] - 341s 11s/step - loss: 0.2134 - accuracy: 0.6146 - f1_m: 0.7830 - precision_m: 0.8316 - recall_m: 0.7495 - val_loss: 0.5618 - val_accuracy: 0.4248 - val_f1_m: 0.5390 - val_precision_m: 0.5572 - val_recall_m: 0.5268\n",
      "\n",
      "Epoch 00016: val_f1_m did not improve from 0.59611\n",
      "Epoch 17/30\n",
      "30/30 [==============================] - 341s 11s/step - loss: 0.2041 - accuracy: 0.6093 - f1_m: 0.7889 - precision_m: 0.8335 - recall_m: 0.7649 - val_loss: 0.5039 - val_accuracy: 0.4690 - val_f1_m: 0.5537 - val_precision_m: 0.7102 - val_recall_m: 0.4643\n",
      "\n",
      "Epoch 00017: val_f1_m did not improve from 0.59611\n",
      "Epoch 18/30\n",
      "30/30 [==============================] - 339s 11s/step - loss: 0.1840 - accuracy: 0.6476 - f1_m: 0.8189 - precision_m: 0.8451 - recall_m: 0.8046 - val_loss: 0.5753 - val_accuracy: 0.4336 - val_f1_m: 0.5196 - val_precision_m: 0.6048 - val_recall_m: 0.4598\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.59611\n",
      "Epoch 19/30\n",
      "30/30 [==============================] - 340s 11s/step - loss: 0.1850 - accuracy: 0.6306 - f1_m: 0.8263 - precision_m: 0.8479 - recall_m: 0.8135 - val_loss: 0.6204 - val_accuracy: 0.4248 - val_f1_m: 0.5707 - val_precision_m: 0.6207 - val_recall_m: 0.5290\n",
      "\n",
      "Epoch 00019: val_f1_m did not improve from 0.59611\n",
      "Epoch 20/30\n",
      "30/30 [==============================] - 345s 11s/step - loss: 0.1621 - accuracy: 0.6348 - f1_m: 0.8417 - precision_m: 0.8724 - recall_m: 0.8267 - val_loss: 0.5630 - val_accuracy: 0.4602 - val_f1_m: 0.5359 - val_precision_m: 0.6708 - val_recall_m: 0.4565\n",
      "\n",
      "Epoch 00020: val_f1_m did not improve from 0.59611\n",
      "Epoch 21/30\n",
      "30/30 [==============================] - 344s 11s/step - loss: 0.1439 - accuracy: 0.6518 - f1_m: 0.8683 - precision_m: 0.8963 - recall_m: 0.8567 - val_loss: 0.7107 - val_accuracy: 0.4248 - val_f1_m: 0.5362 - val_precision_m: 0.6064 - val_recall_m: 0.4844\n",
      "\n",
      "Epoch 00021: val_f1_m did not improve from 0.59611\n",
      "Epoch 22/30\n",
      "30/30 [==============================] - 339s 11s/step - loss: 0.1348 - accuracy: 0.6274 - f1_m: 0.8652 - precision_m: 0.8872 - recall_m: 0.8511 - val_loss: 0.7175 - val_accuracy: 0.4336 - val_f1_m: 0.5226 - val_precision_m: 0.6457 - val_recall_m: 0.4420\n",
      "\n",
      "Epoch 00022: val_f1_m did not improve from 0.59611\n",
      "Epoch 23/30\n",
      "30/30 [==============================] - 342s 11s/step - loss: 0.1180 - accuracy: 0.6529 - f1_m: 0.8836 - precision_m: 0.8925 - recall_m: 0.8782 - val_loss: 0.7718 - val_accuracy: 0.4071 - val_f1_m: 0.5081 - val_precision_m: 0.7170 - val_recall_m: 0.3996\n",
      "\n",
      "Epoch 00023: val_f1_m did not improve from 0.59611\n",
      "Epoch 24/30\n",
      "30/30 [==============================] - 338s 11s/step - loss: 0.1290 - accuracy: 0.6614 - f1_m: 0.8741 - precision_m: 0.8878 - recall_m: 0.8732 - val_loss: 0.7783 - val_accuracy: 0.3894 - val_f1_m: 0.5051 - val_precision_m: 0.6936 - val_recall_m: 0.4018\n",
      "\n",
      "Epoch 00024: val_f1_m did not improve from 0.59611\n",
      "Epoch 25/30\n",
      "30/30 [==============================] - 341s 11s/step - loss: 0.1017 - accuracy: 0.6497 - f1_m: 0.9081 - precision_m: 0.9310 - recall_m: 0.8908 - val_loss: 0.9538 - val_accuracy: 0.3982 - val_f1_m: 0.5489 - val_precision_m: 0.5655 - val_recall_m: 0.5368\n",
      "\n",
      "Epoch 00025: val_f1_m did not improve from 0.59611\n",
      "Epoch 26/30\n",
      "30/30 [==============================] - 351s 12s/step - loss: 0.0922 - accuracy: 0.6614 - f1_m: 0.9119 - precision_m: 0.9254 - recall_m: 0.9057 - val_loss: 0.8882 - val_accuracy: 0.4071 - val_f1_m: 0.5084 - val_precision_m: 0.5957 - val_recall_m: 0.4453\n",
      "\n",
      "Epoch 00026: val_f1_m did not improve from 0.59611\n",
      "Epoch 27/30\n",
      "30/30 [==============================] - 337s 11s/step - loss: 0.0770 - accuracy: 0.6614 - f1_m: 0.9305 - precision_m: 0.9326 - recall_m: 0.9303 - val_loss: 0.7801 - val_accuracy: 0.4425 - val_f1_m: 0.5367 - val_precision_m: 0.6131 - val_recall_m: 0.4821\n",
      "\n",
      "Epoch 00027: val_f1_m did not improve from 0.59611\n",
      "Epoch 28/30\n",
      "30/30 [==============================] - 337s 11s/step - loss: 0.0661 - accuracy: 0.6783 - f1_m: 0.9443 - precision_m: 0.9566 - recall_m: 0.9345 - val_loss: 0.9622 - val_accuracy: 0.4071 - val_f1_m: 0.4815 - val_precision_m: 0.6424 - val_recall_m: 0.3862\n",
      "\n",
      "Epoch 00028: val_f1_m did not improve from 0.59611\n",
      "Epoch 29/30\n",
      "30/30 [==============================] - 337s 11s/step - loss: 0.0566 - accuracy: 0.6709 - f1_m: 0.9489 - precision_m: 0.9561 - recall_m: 0.9437 - val_loss: 1.1152 - val_accuracy: 0.3982 - val_f1_m: 0.5043 - val_precision_m: 0.5962 - val_recall_m: 0.4386\n",
      "\n",
      "Epoch 00029: val_f1_m did not improve from 0.59611\n",
      "Epoch 30/30\n",
      "30/30 [==============================] - 338s 11s/step - loss: 0.0597 - accuracy: 0.6582 - f1_m: 0.9490 - precision_m: 0.9488 - recall_m: 0.9512 - val_loss: 0.9341 - val_accuracy: 0.4513 - val_f1_m: 0.4934 - val_precision_m: 0.6623 - val_recall_m: 0.3996\n",
      "\n",
      "Epoch 00030: val_f1_m did not improve from 0.59611\n",
      "Test accuracy: 60.75%\n",
      "F1 score: 0.63\n",
      "Precision: 0.59\n",
      "Recall: 0.69\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "\n",
    "    #Create CNN model\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    # compile the model\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.0000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_transformer_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = 4\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None), name=\"input\")\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "\n",
    "    x = layers.Dense(units=embed_dim, activation='gelu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, None)]      0         \n",
      "_________________________________________________________________\n",
      "frame_position_embedding (Po (None, None, 1024)        131072    \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 1024)        4211716   \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, None, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "layer_normalization_16 (Laye (None, None, 1024)        2048      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 5,398,536\n",
      "Trainable params: 5,398,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test accuracy: 51.4%\n",
      "F1 score: 0.57\n",
      "Precision: 0.56\n",
      "Recall: 0.59\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 222, 222, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 111, 111, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 111, 111, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 111, 111, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 109, 109, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 54, 54, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 52, 52, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 512)               44302848  \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 44,619,588\n",
      "Trainable params: 44,619,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test accuracy: 50.47%\n",
      "F1 score: 0.33\n",
      "Precision: 0.24\n",
      "Recall: 0.57\n"
     ]
    }
   ],
   "source": [
    "# filepath = os.getcwd() + \"/tmp_3_4/video_classifier\"\n",
    "filepath = os.getcwd() + \"/video_chkpt/video_classifier\"\n",
    "transformer = get_transformer_model()\n",
    "transformer.load_weights(filepath)\n",
    "\n",
    "# evaluate the transformer model\n",
    "loss, accuracy, f1_score, precision, recall = transformer.evaluate(test_image_data, test_labels, verbose=0)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "print(f\"Precision: {round(precision, 2)}\")\n",
    "print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "# filepath = os.getcwd() + \"/temp/audio_classifier\"\n",
    "filepath = os.getcwd() + \"/audio_chkpt/audio_classifier\"\n",
    "cnn = get_cnn_model()\n",
    "cnn.load_weights(filepath)\n",
    "\n",
    "# evaluate the cnn model\n",
    "loss, accuracy, f1_score, precision, recall = cnn.evaluate(test_audio_data, test_labels, verbose=0)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "print(f\"Precision: {round(precision, 2)}\")\n",
    "print(f\"Recall: {round(recall, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_late_fusion():\n",
    "    ## Extract the probabilities from each classifier for the late fusion\n",
    "    res1 = transformer.predict(test_image_data)\n",
    "    # print(res1)\n",
    "    res2 = cnn.predict(test_audio_data)\n",
    "    # print(res2)\n",
    "    all_res = np.array([res1,res2])\n",
    "    all_res = all_res.sum(0)\n",
    "    return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Computing final prediction with late fusion without training\n",
    "# results1 = all_res.sum(0).argmax(1)\n",
    "# results2 = all_res.prod(0).argmax(1)\n",
    "# results3 = np.median(all_res, 0).argmax(1)\n",
    "# results4 = np.max(all_res, 0).argmax(1)\n",
    "\n",
    "# results = all_res.sum(0)\n",
    "\n",
    "\n",
    "def predictLabelForGivenThreshold(results, threshold):\n",
    "    # y_pred=[]\n",
    "    # for sample in results:\n",
    "    #     y_pred.append([1 if i>=threshold else 0 for i in sample ] )\n",
    "    # return np.array(y_pred)\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "    for key,values in enumerate(list(results)):\n",
    "        temp = []\n",
    "        for v in values:\n",
    "            v = (v >= threshold).astype(int)\n",
    "            temp.append(v)\n",
    "        predictions.append(temp) \n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Mature       0.24      1.00      0.38        24\n",
      "   Slapstick       0.42      0.28      0.33        18\n",
      "        Gory       0.30      0.50      0.37         6\n",
      "     Sarcasm       0.54      0.86      0.67        43\n",
      "\n",
      "   micro avg       0.36      0.76      0.49        91\n",
      "   macro avg       0.37      0.66      0.44        91\n",
      "weighted avg       0.42      0.76      0.51        91\n",
      " samples avg       0.36      0.55      0.42        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "label_names = ['Mature', 'Slapstick', 'Gory', 'Sarcasm']\n",
    "results = get_late_fusion()\n",
    "y_pred  = predictLabelForGivenThreshold(results,0.7)\n",
    "print(classification_report(test_labels, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 of each label: [0.38095238 0.33333333 0.375      0.66666667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"F1 of each label: {}\".format(metrics.f1_score(test_labels, y_pred, average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34112149532710273"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "Accuracy(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12149532710280374\n",
      "0.12149532710280374\n"
     ]
    }
   ],
   "source": [
    "#Exact match ratio\n",
    "MR = np.all(y_pred == test_labels, axis=1).mean()\n",
    "print(MR)\n",
    "print(accuracy_score(test_labels,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Mature       0.48      0.54      0.51        24\n",
      "   Slapstick       1.00      0.11      0.20        18\n",
      "        Gory       0.27      0.50      0.35         6\n",
      "     Sarcasm       0.72      0.72      0.72        43\n",
      "\n",
      "   micro avg       0.59      0.54      0.56        91\n",
      "   macro avg       0.62      0.47      0.45        91\n",
      "weighted avg       0.68      0.54      0.54        91\n",
      " samples avg       0.41      0.39      0.40        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\maitr\\AppData\\Local\\Temp\\ipykernel_1632\\920900653.py:4: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_res = transformer.predict(test_image_data)\n",
    "img_y_pred = predictLabelForGivenThreshold(img_res,0.6)\n",
    "print(classification_report(test_labels, img_y_pred,target_names=label_names))\n",
    "Accuracy(test_labels, img_y_pred)\n",
    "# print(img_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Mature       0.22      1.00      0.37        24\n",
      "   Slapstick       0.18      1.00      0.31        18\n",
      "        Gory       0.00      0.00      0.00         6\n",
      "     Sarcasm       0.40      1.00      0.57        43\n",
      "\n",
      "   micro avg       0.27      0.93      0.42        91\n",
      "   macro avg       0.20      0.75      0.31        91\n",
      "weighted avg       0.28      0.93      0.43        91\n",
      " samples avg       0.27      0.68      0.38        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26791277258566965"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_res = cnn.predict(test_audio_data)\n",
    "audio_y_pred = predictLabelForGivenThreshold(audio_res,0.1)\n",
    "print(classification_report(test_labels, audio_y_pred,target_names=label_names))\n",
    "Accuracy(test_labels, audio_y_pred)\n",
    "# print(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('extracted_test_spectrogram/al-TxOuSqc8.02.jpg')\n",
    "# img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "# img = np.array(img)\n",
    "# # img = test_data[104]\n",
    "# img = img.reshape((1,IMG_SIZE,IMG_SIZE,3))\n",
    "# # print(img.shape)\n",
    "\n",
    "# y_pred = cnn.predict(img)[0]\n",
    "\n",
    "# # # round probabilities to class labels\n",
    "# # y_pred = y_pred.round()\n",
    "\n",
    "\n",
    "# print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9abb609d9af7c865ec2d4837dadf5771495602565493ad3e886cad7ec3618278"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf.2.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
