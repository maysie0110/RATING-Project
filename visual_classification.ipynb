{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "37db3b7a-c81d-4f6a-b8a8-9f5674c6feb5",
    "deepnote_cell_type": "text-cell-h1",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# RATING Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4c574a17-beaa-467a-80e8-64c8c9578cbc",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Comic Mischief Detection Task\n",
    "\n",
    "Files:\n",
    "\n",
    "1. \"train.csv\" : \n",
    "-- Contains multiclass classification content annotations for each video scene used in the training set.\n",
    "-- Annotations are on a scene level and do not correspond to a specific modality\n",
    "-- a \".csv\" file containing video URLs as well as the IDs of the scenes used in the training set.\n",
    "-- Videos are available in the form of URLs, collected from the Youtube and the IMDB websites.\n",
    "-- Contains metadata about the videos.\n",
    "-- Four content categories related to comic mischief are used (Sarcasm, Slapstick Humor, Gory Humor, Mature Humor).\n",
    "\n",
    "2. \"val.csv\" : \n",
    "-- Contains multiclass classification content annotations for each video scene used in the validation set.\n",
    "-- You can use this set for performing model hyperparameter tuning before using the test set\n",
    "\n",
    "\n",
    "3. \"test.csv\" : \n",
    "-- Contains multiclass classification content annotations for each video scene used in the test set.\n",
    "-- You can use this set for evaluating your method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "ae028c11-f4b1-4180-9f60-74f25891fb1f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3894,
    "execution_start": 1644814279553,
    "source_hash": "94243ef8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "09480eaa-48a0-439d-b679-23024487d866",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1644814284695,
    "source_hash": "2000a060",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## global variables\n",
    "\n",
    "train_dir = os.getcwd() + \"/train_data/\"\n",
    "val_dir = os.getcwd() + \"/val_data/\"\n",
    "test_dir = os.getcwd() + \"/test_data/\"\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_SEQ_LENGTH = 60\n",
    "FRAME_GAP = 24\n",
    "NUM_FEATURES = 1024\n",
    "IMG_SIZE = 224\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fdb0968c-cba8-478b-852c-4389220f825c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### References\n",
    "1. https://keras.io/examples/vision/video_transformers/)\n",
    "2. https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub)\n",
    "3. https://colab.research.google.com/github/sayakpaul/Action-Recognition-in-TensorFlow/blob/main/Data_Preparation_UCF101.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c5c164a6-1b18-4f27-a32a-1fef09258dbb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "be3d8688-9ac5-4e29-9e99-f13b154a58ac",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### METADATA loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "63c68a51-5839-4d9b-9d51-4890d0bed7e1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 119,
    "execution_start": 1644814288992,
    "source_hash": "68973930",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Scene_ID</th>\n",
       "      <th>Video URL</th>\n",
       "      <th>Codec</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Avg Frame rate</th>\n",
       "      <th>Mature Humor - Scene Annotation</th>\n",
       "      <th>Slapstick Humor - Scene Annotation</th>\n",
       "      <th>Gory Humor - Scene Annotation</th>\n",
       "      <th>Sarcasm - Scene Annotation</th>\n",
       "      <th>combination</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt2872718</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.imdb.com/videoplayer/vi4179799833</td>\n",
       "      <td>h264</td>\n",
       "      <td>854 x 480</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt2872718</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.imdb.com/videoplayer/vi4179799833</td>\n",
       "      <td>h264</td>\n",
       "      <td>854 x 480</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0010</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt2788710</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.imdb.com/videoplayer/vi1114222361</td>\n",
       "      <td>h264</td>\n",
       "      <td>854 x 480</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt2788710</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.imdb.com/videoplayer/vi1114222361</td>\n",
       "      <td>h264</td>\n",
       "      <td>854 x 480</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt2788710</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.imdb.com/videoplayer/vi1114222361</td>\n",
       "      <td>h264</td>\n",
       "      <td>854 x 480</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Video ID  Scene_ID                                       Video URL Codec  \\\n",
       "0  tt2872718         0   https://www.imdb.com/videoplayer/vi4179799833  h264   \n",
       "1  tt2872718         1   https://www.imdb.com/videoplayer/vi4179799833  h264   \n",
       "2  tt2788710         0   https://www.imdb.com/videoplayer/vi1114222361  h264   \n",
       "3  tt2788710         1   https://www.imdb.com/videoplayer/vi1114222361  h264   \n",
       "4  tt2788710         2   https://www.imdb.com/videoplayer/vi1114222361  h264   \n",
       "\n",
       "  Resolution  Avg Frame rate  Mature Humor - Scene Annotation  \\\n",
       "0  854 x 480       23.976024                                0   \n",
       "1  854 x 480       23.976024                                0   \n",
       "2  854 x 480       23.976024                                0   \n",
       "3  854 x 480       23.976024                                1   \n",
       "4  854 x 480       23.976024                                0   \n",
       "\n",
       "   Slapstick Humor - Scene Annotation  Gory Humor - Scene Annotation  \\\n",
       "0                                   0                              0   \n",
       "1                                   0                              1   \n",
       "2                                   0                              0   \n",
       "3                                   0                              0   \n",
       "4                                   0                              0   \n",
       "\n",
       "   Sarcasm - Scene Annotation combination  \\\n",
       "0                           0        0000   \n",
       "1                           0        0010   \n",
       "2                           0        0000   \n",
       "3                           0        1000   \n",
       "4                           0        0000   \n",
       "\n",
       "                                                path  \n",
       "0  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "1  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "2  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "3  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "4  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the training set.\n",
    "train_df = pd.read_csv('train-updated.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "train_df[\"path\"] = train_dir + train_df[\"Video ID\"]+ \".0\" + train_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "f080ba32-7528-4f94-98aa-0fd621590003",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 70,
    "execution_start": 1644814290089,
    "source_hash": "b3c9ae0a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Scene_ID</th>\n",
       "      <th>Video URL</th>\n",
       "      <th>Codec</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Avg Frame rate</th>\n",
       "      <th>Mature Humor - Scene Annotation</th>\n",
       "      <th>Slapstick Humor - Scene Annotation</th>\n",
       "      <th>Gory Humor - Scene Annotation</th>\n",
       "      <th>Sarcasm - Scene Annotation</th>\n",
       "      <th>combination</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt1308728</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.youtube.com/watch?v=QP9qbhTeBII</td>\n",
       "      <td>h264</td>\n",
       "      <td>640 x 360</td>\n",
       "      <td>23.975945</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1308728</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.youtube.com/watch?v=QP9qbhTeBII</td>\n",
       "      <td>h264</td>\n",
       "      <td>640 x 360</td>\n",
       "      <td>23.975945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1100</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt1308728</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.youtube.com/watch?v=QP9qbhTeBII</td>\n",
       "      <td>h264</td>\n",
       "      <td>640 x 360</td>\n",
       "      <td>23.975945</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PGuqnE35cCg</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.youtube.com/watch?v=PGuqnE35cCg</td>\n",
       "      <td>h264</td>\n",
       "      <td>640 x 360</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PGuqnE35cCg</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.youtube.com/watch?v=PGuqnE35cCg</td>\n",
       "      <td>h264</td>\n",
       "      <td>640 x 360</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Video ID  Scene_ID                                     Video URL Codec  \\\n",
       "0    tt1308728         0   https://www.youtube.com/watch?v=QP9qbhTeBII  h264   \n",
       "1    tt1308728         1   https://www.youtube.com/watch?v=QP9qbhTeBII  h264   \n",
       "2    tt1308728         2   https://www.youtube.com/watch?v=QP9qbhTeBII  h264   \n",
       "3  PGuqnE35cCg         0   https://www.youtube.com/watch?v=PGuqnE35cCg  h264   \n",
       "4  PGuqnE35cCg         1   https://www.youtube.com/watch?v=PGuqnE35cCg  h264   \n",
       "\n",
       "  Resolution  Avg Frame rate  Mature Humor - Scene Annotation  \\\n",
       "0  640 x 360       23.975945                                1   \n",
       "1  640 x 360       23.975945                                1   \n",
       "2  640 x 360       23.975945                                1   \n",
       "3  640 x 360       23.976024                                1   \n",
       "4  640 x 360       23.976024                                1   \n",
       "\n",
       "   Slapstick Humor - Scene Annotation  Gory Humor - Scene Annotation  \\\n",
       "0                                   0                              0   \n",
       "1                                   1                              0   \n",
       "2                                   0                              0   \n",
       "3                                   0                              0   \n",
       "4                                   0                              0   \n",
       "\n",
       "   Sarcasm - Scene Annotation combination  \\\n",
       "0                           0        1000   \n",
       "1                           0        1100   \n",
       "2                           0        1000   \n",
       "3                           0        1000   \n",
       "4                           0        1000   \n",
       "\n",
       "                                                path  \n",
       "0  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "1  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "2  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "3  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "4  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the validation set.\n",
    "val_df = pd.read_csv('val.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "val_df[\"path\"] = val_dir + val_df[\"Video ID\"]+ \".0\" + val_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "e2e0a169-b660-4bc0-b85d-e226eb0efde2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 81,
    "execution_start": 1644814291719,
    "source_hash": "8b166588",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Scene_ID</th>\n",
       "      <th>Video URL</th>\n",
       "      <th>Codec</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Avg Frame rate</th>\n",
       "      <th>Mature Humor - Scene Annotation</th>\n",
       "      <th>Slapstick Humor - Scene Annotation</th>\n",
       "      <th>Gory Humor - Scene Annotation</th>\n",
       "      <th>Sarcasm - Scene Annotation</th>\n",
       "      <th>combination</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt1741243</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.imdb.com/videoplayer/vi2169153049</td>\n",
       "      <td>h264</td>\n",
       "      <td>534 x 360</td>\n",
       "      <td>29.970000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1741243</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.imdb.com/videoplayer/vi2169153049</td>\n",
       "      <td>h264</td>\n",
       "      <td>534 x 360</td>\n",
       "      <td>29.970000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0110</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt1723121</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.youtube.com/watch?v=O7NHfAzg7Yg</td>\n",
       "      <td>h264</td>\n",
       "      <td>640 x 360</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt1723121</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.youtube.com/watch?v=O7NHfAzg7Yg</td>\n",
       "      <td>h264</td>\n",
       "      <td>640 x 360</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt1723121</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.youtube.com/watch?v=O7NHfAzg7Yg</td>\n",
       "      <td>h264</td>\n",
       "      <td>640 x 360</td>\n",
       "      <td>23.976024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Video ID  Scene_ID                                       Video URL Codec  \\\n",
       "0  tt1741243         0   https://www.imdb.com/videoplayer/vi2169153049  h264   \n",
       "1  tt1741243         1   https://www.imdb.com/videoplayer/vi2169153049  h264   \n",
       "2  tt1723121         0     https://www.youtube.com/watch?v=O7NHfAzg7Yg  h264   \n",
       "3  tt1723121         1     https://www.youtube.com/watch?v=O7NHfAzg7Yg  h264   \n",
       "4  tt1723121         2     https://www.youtube.com/watch?v=O7NHfAzg7Yg  h264   \n",
       "\n",
       "  Resolution  Avg Frame rate  Mature Humor - Scene Annotation  \\\n",
       "0  534 x 360       29.970000                                1   \n",
       "1  534 x 360       29.970000                                0   \n",
       "2  640 x 360       23.976024                                1   \n",
       "3  640 x 360       23.976024                                1   \n",
       "4  640 x 360       23.976024                                1   \n",
       "\n",
       "   Slapstick Humor - Scene Annotation  Gory Humor - Scene Annotation  \\\n",
       "0                                   0                              0   \n",
       "1                                   1                              1   \n",
       "2                                   0                              0   \n",
       "3                                   0                              0   \n",
       "4                                   0                              0   \n",
       "\n",
       "   Sarcasm - Scene Annotation combination  \\\n",
       "0                           0        1000   \n",
       "1                           0        0110   \n",
       "2                           0        1000   \n",
       "3                           0        1000   \n",
       "4                           0        1000   \n",
       "\n",
       "                                                path  \n",
       "0  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "1  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "2  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "3  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  \n",
       "4  c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the test set.\n",
    "test_df = pd.read_csv('test-updated.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "test_df[\"path\"] = test_dir + test_df[\"Video ID\"]+ \".0\" + test_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "14f1fcca-b038-4f54-b98e-e54e45138366",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "07f90c3b-a279-4ab5-9501-e37a9bbb0c35",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "How to feed videos to a neural network for training? <br>\n",
    "\n",
    "1. Use OpenCV VideoCapture() method to read frames from videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_id": "81e2aa7b-587b-4e06-ab22-3f66909bb3ab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1644814294456,
    "source_hash": "f99f02",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utilities to open video files using CV2\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    count = 0\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if count % FRAME_GAP == 0:\n",
    "                frame = crop_center_square(frame)\n",
    "                frame = cv2.resize(frame, resize)\n",
    "                frame = frame[:, :, [2, 1, 0]]\n",
    "                frames.append(frame)\n",
    "\n",
    "            count=count+1\n",
    "            \n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a68d340b-5464-4ca5-bc55-45a3d43ced3c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Feature Extraction with pre-trained DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_id": "f786b771-abbd-49f7-8277-f9797e3db451",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4579,
    "execution_start": 1644814297807,
    "source_hash": "a12163ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.DenseNet121(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.densenet.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "80c029b6-f188-42de-8373-40e907fadd59",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Label Preprocessing \n",
    "\n",
    "Multilabel classification -->  Multi-class Binarization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_id": "65203327-f60b-42f0-911d-7b460f2edebe",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 136,
    "execution_start": 1644814305741,
    "source_hash": "32006f2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(942, 4)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for x in train_df[\"combination\"].values:\n",
    "    lst = list(map(int, x))\n",
    "    arr = np.asarray(lst)\n",
    "    labels.append(arr)\n",
    "labels = np.reshape(labels,(len(labels),4))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_id": "a4d34440-9081-4284-8d0a-1bdba0082008",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1644814396696,
    "source_hash": "52154926",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"path\"].values.tolist()\n",
    "\n",
    "    labels = []\n",
    "    for x in df[\"combination\"].values:\n",
    "        lst = list(map(int, x))\n",
    "        arr = np.asarray(lst)\n",
    "        labels.append(arr)\n",
    "    labels = np.reshape(labels,(len(labels),4))\n",
    "\n",
    "    # `frame_features` are what we will feed to our sequence model.\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(path)\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(path)\n",
    "\n",
    "        # Pad shorter videos.\n",
    "        if len(frames) < MAX_SEQ_LENGTH:\n",
    "            diff = MAX_SEQ_LENGTH - len(frames)\n",
    "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
    "            frames = np.concatenate((frames, padding))\n",
    "\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholder to store the features of the current video.\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                if np.mean(batch[j, :]) > 0.0:\n",
    "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                        batch[None, j, :]\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    temp_frame_features[i, j, :] = 0.0\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "\n",
    "    return frame_features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b428fb81-cd25-48f5-9742-1f7ea202064d",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Build the Transformer-based model - BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "e36a7e79-836f-4942-a832-d53954293523",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "51248883",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "3e2e67d3-0540-476b-b812-04d019aa8269",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "521c40ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subclassed layer\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def get_compiled_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    # classes = len(label_processor.get_vocabulary())\n",
    "    classes = 4\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None))\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ae7a96a5-1e99-4eb3-8183-430d4b85f098",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Model Training and Testing (max seq length = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    log_dir = \"logs/fit/video_chkpt_128\" \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    filepath = os.getcwd() + \"/seq_length_128/video_chkpt_128/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, \n",
    "        monitor='val_f1_m',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    with tf.device('/device:CPU:0'):\n",
    "        model = get_compiled_model()\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            train_labels,\n",
    "            validation_data=(val_data,val_labels),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint, tensorboard_callback],\n",
    "        )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    # _, accuracy = model.evaluate(test_data, test_labels)\n",
    "    # evaluate the model\n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "    print(f\"Precision: {round(precision, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (942, 128, 1024)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = np.load(\"seq_length_128/extracted_data/train_data.npy\"), np.load(\"seq_length_128/extracted_data/train_labels.npy\")\n",
    "val_data, val_labels = np.load(\"seq_length_128/extracted_data/val_data.npy\"), np.load(\"seq_length_128/extracted_data/val_labels.npy\")\n",
    "test_data, test_labels = np.load(\"seq_length_128/extracted_data/test_data.npy\"), np.load(\"seq_length_128/extracted_data/test_labels.npy\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, None, None)]      0         \n",
      "_________________________________________________________________\n",
      "frame_position_embedding (Po (None, None, 1024)        131072    \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 1024)        4211716   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,346,888\n",
      "Trainable params: 4,346,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer PositionalEmbedding has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 22s 708ms/step - loss: 0.8070 - accuracy: 0.3142 - f1_m: 0.2386 - precision_m: 0.3027 - recall_m: 0.2204 - val_loss: 0.6277 - val_accuracy: 0.2301 - val_f1_m: 0.4023 - val_precision_m: 0.3886 - val_recall_m: 0.4208\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.40228, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_128/video_chkpt_128\\video_classifier\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 18s 593ms/step - loss: 0.5689 - accuracy: 0.3535 - f1_m: 0.3068 - precision_m: 0.4093 - recall_m: 0.2562 - val_loss: 0.5527 - val_accuracy: 0.2743 - val_f1_m: 0.2080 - val_precision_m: 0.3777 - val_recall_m: 0.1529\n",
      "\n",
      "Epoch 00002: val_f1_m did not improve from 0.40228\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 19s 625ms/step - loss: 0.5258 - accuracy: 0.3981 - f1_m: 0.3783 - precision_m: 0.5059 - recall_m: 0.3209 - val_loss: 0.4826 - val_accuracy: 0.4336 - val_f1_m: 0.1991 - val_precision_m: 0.4671 - val_recall_m: 0.1373\n",
      "\n",
      "Epoch 00003: val_f1_m did not improve from 0.40228\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 18s 588ms/step - loss: 0.4890 - accuracy: 0.4214 - f1_m: 0.4145 - precision_m: 0.5362 - recall_m: 0.3490 - val_loss: 0.4985 - val_accuracy: 0.4248 - val_f1_m: 0.1547 - val_precision_m: 0.3172 - val_recall_m: 0.1094\n",
      "\n",
      "Epoch 00004: val_f1_m did not improve from 0.40228\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 18s 598ms/step - loss: 0.4596 - accuracy: 0.4268 - f1_m: 0.4075 - precision_m: 0.5774 - recall_m: 0.3453 - val_loss: 0.4786 - val_accuracy: 0.3805 - val_f1_m: 0.2147 - val_precision_m: 0.4139 - val_recall_m: 0.1484\n",
      "\n",
      "Epoch 00005: val_f1_m did not improve from 0.40228\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 18s 595ms/step - loss: 0.4164 - accuracy: 0.4862 - f1_m: 0.4892 - precision_m: 0.6186 - recall_m: 0.4190 - val_loss: 0.4789 - val_accuracy: 0.3628 - val_f1_m: 0.3771 - val_precision_m: 0.6062 - val_recall_m: 0.2835\n",
      "\n",
      "Epoch 00006: val_f1_m did not improve from 0.40228\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 18s 595ms/step - loss: 0.4090 - accuracy: 0.4830 - f1_m: 0.5114 - precision_m: 0.6299 - recall_m: 0.4431 - val_loss: 0.5163 - val_accuracy: 0.3805 - val_f1_m: 0.2647 - val_precision_m: 0.3800 - val_recall_m: 0.2042\n",
      "\n",
      "Epoch 00007: val_f1_m did not improve from 0.40228\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 18s 594ms/step - loss: 0.4134 - accuracy: 0.4989 - f1_m: 0.4691 - precision_m: 0.6069 - recall_m: 0.4072 - val_loss: 0.4954 - val_accuracy: 0.4159 - val_f1_m: 0.5096 - val_precision_m: 0.5400 - val_recall_m: 0.4978\n",
      "\n",
      "Epoch 00008: val_f1_m improved from 0.40228 to 0.50962, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_128/video_chkpt_128\\video_classifier\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 18s 598ms/step - loss: 0.3666 - accuracy: 0.5000 - f1_m: 0.5466 - precision_m: 0.6625 - recall_m: 0.4830 - val_loss: 0.5346 - val_accuracy: 0.3717 - val_f1_m: 0.2528 - val_precision_m: 0.6042 - val_recall_m: 0.1808\n",
      "\n",
      "Epoch 00009: val_f1_m did not improve from 0.50962\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 17s 571ms/step - loss: 0.3486 - accuracy: 0.5276 - f1_m: 0.5671 - precision_m: 0.6982 - recall_m: 0.5021 - val_loss: 0.5500 - val_accuracy: 0.3451 - val_f1_m: 0.3776 - val_precision_m: 0.4649 - val_recall_m: 0.3237\n",
      "\n",
      "Epoch 00010: val_f1_m did not improve from 0.50962\n",
      "Test accuracy: 54.21%\n",
      "F1 score: 0.59\n",
      "Precision: 0.54\n",
      "Recall: 0.69\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and testing (max seq length = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7327af42-8a03-4c46-b38e-e6931aa020f3' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (942, 60, 1024)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = np.load(\"seq_length_60/extracted_data/train_data.npy\"), np.load(\"seq_length_60/extracted_data/train_labels.npy\")\n",
    "val_data, val_labels = np.load(\"seq_length_60/extracted_data/val_data.npy\"), np.load(\"seq_length_60/extracted_data/val_labels.npy\")\n",
    "test_data, test_labels = np.load(\"seq_length_60/extracted_data/test_data.npy\"), np.load(\"seq_length_60/extracted_data/test_labels.npy\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_60():\n",
    "    log_dir = \"logs/fit/video_chkpt_60\" \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    filepath = os.getcwd() + \"/seq_length_60/video_chkpt_60_2/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, \n",
    "        monitor='val_f1_m',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    with tf.device('/device:CPU:0'):\n",
    "        model = get_compiled_model()\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            train_labels,\n",
    "            validation_data=(val_data,val_labels),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint, tensorboard_callback],\n",
    "        )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    # _, accuracy = model.evaluate(test_data, test_labels)\n",
    "    # evaluate the model\n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "    print(f\"Precision: {round(precision, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, None, None)]      0         \n",
      "_________________________________________________________________\n",
      "frame_position_embedding (Po (None, None, 1024)        61440     \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 1024)        4211716   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,277,256\n",
      "Trainable params: 4,277,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer PositionalEmbedding has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 11s 338ms/step - loss: 0.9289 - accuracy: 0.2972 - f1_m: 0.2373 - precision_m: 0.3269 - recall_m: 0.2376 - val_loss: 0.5221 - val_accuracy: 0.2301 - val_f1_m: 0.0601 - val_precision_m: 0.5000 - val_recall_m: 0.0324\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.06010, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_60/video_chkpt_60_2\\video_classifier\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 9s 288ms/step - loss: 0.5975 - accuracy: 0.3068 - f1_m: 0.2288 - precision_m: 0.3314 - recall_m: 0.1818 - val_loss: 0.5909 - val_accuracy: 0.2301 - val_f1_m: 0.4059 - val_precision_m: 0.3872 - val_recall_m: 0.4286\n",
      "\n",
      "Epoch 00002: val_f1_m improved from 0.06010 to 0.40586, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_60/video_chkpt_60_2\\video_classifier\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 9s 289ms/step - loss: 0.5758 - accuracy: 0.3142 - f1_m: 0.2498 - precision_m: 0.3488 - recall_m: 0.2027 - val_loss: 0.5124 - val_accuracy: 0.2743 - val_f1_m: 0.1231 - val_precision_m: 0.3750 - val_recall_m: 0.0748\n",
      "\n",
      "Epoch 00003: val_f1_m did not improve from 0.40586\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 8s 270ms/step - loss: 0.5116 - accuracy: 0.3981 - f1_m: 0.3578 - precision_m: 0.4910 - recall_m: 0.2903 - val_loss: 0.5249 - val_accuracy: 0.3805 - val_f1_m: 0.3937 - val_precision_m: 0.4660 - val_recall_m: 0.3694\n",
      "\n",
      "Epoch 00004: val_f1_m did not improve from 0.40586\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 8s 266ms/step - loss: 0.4924 - accuracy: 0.4257 - f1_m: 0.4050 - precision_m: 0.5365 - recall_m: 0.3412 - val_loss: 0.5391 - val_accuracy: 0.3628 - val_f1_m: 0.4088 - val_precision_m: 0.4917 - val_recall_m: 0.3873\n",
      "\n",
      "Epoch 00005: val_f1_m improved from 0.40586 to 0.40876, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_60/video_chkpt_60_2\\video_classifier\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 8s 268ms/step - loss: 0.4570 - accuracy: 0.4427 - f1_m: 0.3945 - precision_m: 0.5406 - recall_m: 0.3272 - val_loss: 0.4697 - val_accuracy: 0.4071 - val_f1_m: 0.2752 - val_precision_m: 0.5191 - val_recall_m: 0.1975\n",
      "\n",
      "Epoch 00006: val_f1_m did not improve from 0.40876\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 8s 264ms/step - loss: 0.4280 - accuracy: 0.4735 - f1_m: 0.4507 - precision_m: 0.5761 - recall_m: 0.3832 - val_loss: 0.4842 - val_accuracy: 0.3894 - val_f1_m: 0.2233 - val_precision_m: 0.4500 - val_recall_m: 0.1540\n",
      "\n",
      "Epoch 00007: val_f1_m did not improve from 0.40876\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 8s 260ms/step - loss: 0.4152 - accuracy: 0.4894 - f1_m: 0.4774 - precision_m: 0.5943 - recall_m: 0.4204 - val_loss: 0.4695 - val_accuracy: 0.3628 - val_f1_m: 0.4534 - val_precision_m: 0.6286 - val_recall_m: 0.3616\n",
      "\n",
      "Epoch 00008: val_f1_m improved from 0.40876 to 0.45342, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_60/video_chkpt_60_2\\video_classifier\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 8s 262ms/step - loss: 0.3886 - accuracy: 0.4873 - f1_m: 0.5270 - precision_m: 0.6823 - recall_m: 0.4437 - val_loss: 0.4741 - val_accuracy: 0.4602 - val_f1_m: 0.3833 - val_precision_m: 0.5764 - val_recall_m: 0.2958\n",
      "\n",
      "Epoch 00009: val_f1_m did not improve from 0.45342\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 8s 262ms/step - loss: 0.3888 - accuracy: 0.5149 - f1_m: 0.5438 - precision_m: 0.6629 - recall_m: 0.4742 - val_loss: 0.5403 - val_accuracy: 0.3097 - val_f1_m: 0.3722 - val_precision_m: 0.5805 - val_recall_m: 0.2879\n",
      "\n",
      "Epoch 00010: val_f1_m did not improve from 0.45342\n",
      "Test accuracy: 56.07%\n",
      "F1 score: 0.53\n",
      "Precision: 0.57\n",
      "Recall: 0.56\n"
     ]
    }
   ],
   "source": [
    "trained_model_60 = run_experiment_60()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Seq Length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (942, 20, 1024)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = np.load(\"seq_length_20/extracted_data/train_data.npy\"), np.load(\"seq_length_20/extracted_data/train_labels.npy\")\n",
    "val_data, val_labels = np.load(\"seq_length_20/extracted_data/val_data.npy\"), np.load(\"seq_length_20/extracted_data/val_labels.npy\")\n",
    "test_data, test_labels = np.load(\"seq_length_20/extracted_data/test_data.npy\"), np.load(\"seq_length_20/extracted_data/test_labels.npy\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_20():\n",
    "    log_dir = \"logs/fit/video_chkpt_20\" \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    filepath = os.getcwd() + \"/seq_length_20/video_chkpt_20/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, \n",
    "        monitor='val_f1_m',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    with tf.device('/device:CPU:0'):\n",
    "        model = get_compiled_model()\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            train_labels,\n",
    "            validation_data=(val_data,val_labels),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint, tensorboard_callback],\n",
    "        )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    # _, accuracy = model.evaluate(test_data, test_labels)\n",
    "    # evaluate the model\n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "    print(f\"Precision: {round(precision, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, None, None)]      0         \n",
      "_________________________________________________________________\n",
      "frame_position_embedding (Po (None, None, 1024)        20480     \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 1024)        4211716   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,236,296\n",
      "Trainable params: 4,236,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer PositionalEmbedding has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 6s 173ms/step - loss: 0.9530 - accuracy: 0.3185 - f1_m: 0.2676 - precision_m: 0.3007 - recall_m: 0.2686 - val_loss: 0.5842 - val_accuracy: 0.2301 - val_f1_m: 0.3975 - val_precision_m: 0.3978 - val_recall_m: 0.4018\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.39752, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_20/video_chkpt_20\\video_classifier\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 0.6279 - accuracy: 0.3323 - f1_m: 0.2851 - precision_m: 0.3625 - recall_m: 0.2457 - val_loss: 0.5694 - val_accuracy: 0.3894 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_f1_m did not improve from 0.39752\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 3s 114ms/step - loss: 0.6092 - accuracy: 0.3673 - f1_m: 0.3311 - precision_m: 0.4078 - recall_m: 0.2931 - val_loss: 0.5364 - val_accuracy: 0.2389 - val_f1_m: 0.4046 - val_precision_m: 0.4544 - val_recall_m: 0.3873\n",
      "\n",
      "Epoch 00003: val_f1_m improved from 0.39752 to 0.40465, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_20/video_chkpt_20\\video_classifier\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 3s 114ms/step - loss: 0.5588 - accuracy: 0.3397 - f1_m: 0.3003 - precision_m: 0.3744 - recall_m: 0.2588 - val_loss: 0.5368 - val_accuracy: 0.3628 - val_f1_m: 0.0692 - val_precision_m: 0.3611 - val_recall_m: 0.0435\n",
      "\n",
      "Epoch 00004: val_f1_m did not improve from 0.40465\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.5245 - accuracy: 0.3673 - f1_m: 0.3528 - precision_m: 0.4562 - recall_m: 0.2953 - val_loss: 0.5617 - val_accuracy: 0.3628 - val_f1_m: 0.3064 - val_precision_m: 0.4568 - val_recall_m: 0.2467\n",
      "\n",
      "Epoch 00005: val_f1_m did not improve from 0.40465\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.4995 - accuracy: 0.3970 - f1_m: 0.3852 - precision_m: 0.4966 - recall_m: 0.3280 - val_loss: 0.4849 - val_accuracy: 0.3894 - val_f1_m: 0.1395 - val_precision_m: 0.3194 - val_recall_m: 0.0893\n",
      "\n",
      "Epoch 00006: val_f1_m did not improve from 0.40465\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 3s 104ms/step - loss: 0.4696 - accuracy: 0.4076 - f1_m: 0.4014 - precision_m: 0.5340 - recall_m: 0.3377 - val_loss: 0.5293 - val_accuracy: 0.3009 - val_f1_m: 0.2548 - val_precision_m: 0.5530 - val_recall_m: 0.1674\n",
      "\n",
      "Epoch 00007: val_f1_m did not improve from 0.40465\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 3s 103ms/step - loss: 0.4427 - accuracy: 0.4724 - f1_m: 0.4822 - precision_m: 0.5785 - recall_m: 0.4215 - val_loss: 0.5150 - val_accuracy: 0.3894 - val_f1_m: 0.2834 - val_precision_m: 0.5444 - val_recall_m: 0.1942\n",
      "\n",
      "Epoch 00008: val_f1_m did not improve from 0.40465\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 3s 103ms/step - loss: 0.4240 - accuracy: 0.4554 - f1_m: 0.4714 - precision_m: 0.6082 - recall_m: 0.3979 - val_loss: 0.5106 - val_accuracy: 0.4071 - val_f1_m: 0.4104 - val_precision_m: 0.5215 - val_recall_m: 0.3527\n",
      "\n",
      "Epoch 00009: val_f1_m improved from 0.40465 to 0.41038, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_20/video_chkpt_20\\video_classifier\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.4050 - accuracy: 0.4841 - f1_m: 0.5041 - precision_m: 0.6137 - recall_m: 0.4467 - val_loss: 0.5674 - val_accuracy: 0.3274 - val_f1_m: 0.4110 - val_precision_m: 0.4717 - val_recall_m: 0.3739\n",
      "\n",
      "Epoch 00010: val_f1_m improved from 0.41038 to 0.41100, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/seq_length_20/video_chkpt_20\\video_classifier\n",
      "Test accuracy: 51.4%\n",
      "F1 score: 0.54\n",
      "Precision: 0.53\n",
      "Recall: 0.58\n"
     ]
    }
   ],
   "source": [
    "trained_model_20 = run_experiment_20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "05ab0b75-b65c-4d2b-b56a-635b53176e13",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
