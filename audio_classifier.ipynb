{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from scipy.io import wavfile # scipy library to read wav files\n",
    "from scipy.fftpack import fft # fourier transform\n",
    "from scipy import signal\n",
    "\n",
    "import librosa.display\n",
    "import pylab\n",
    "import librosa    \n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from utilities import f1_m, recall_m, precision_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "train_dir = os.getcwd() + \"/train_data/\"\n",
    "val_dir = os.getcwd() + \"/val_data/\"\n",
    "test_dir = os.getcwd() + \"/test_data/\"\n",
    "\n",
    "train_audio_dir = os.getcwd() + \"/extracted_train_audio/\"\n",
    "val_audio_dir = os.getcwd() + \"/extracted_val_audio/\"\n",
    "test_audio_dir = os.getcwd() + \"/extracted_test_audio/\"\n",
    "\n",
    "#directory to save extracted data\n",
    "extracted_data_dir = os.getcwd() + \"/extracted_audio_data/\"\n",
    "# os.mkdir(extracted_data_dir)\n",
    "\n",
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the training set.\n",
    "train_df = pd.read_csv('train-updated.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "train_df[\"path\"] = train_dir + train_df[\"Video ID\"]+ \".0\" + train_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "\n",
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the validation set.\n",
    "val_df = pd.read_csv('val.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "val_df[\"path\"] = val_dir + val_df[\"Video ID\"]+ \".0\" + val_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "\n",
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the test set.\n",
    "test_df = pd.read_csv('test-updated.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "test_df[\"path\"] = test_dir + test_df[\"Video ID\"]+ \".0\" + test_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call([\"ffmpeg\", \"-y\", \"-i\", \"tt2872718.00.mp4\", \"test_audio.wav\"], \n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrograms(audio_dir):\n",
    "    # Load the audio wav file into numpy array\n",
    "    audio_data, samplerate = librosa.load(audio_dir)\n",
    "    # print(audio_data.shape)\n",
    "    # print(samplerate)\n",
    "\n",
    "    #Compute a mel-scaled spectrogram\n",
    "    mel_feat = librosa.feature.melspectrogram(y=audio_data, sr=samplerate)\n",
    "    librosa.display.specshow(mel_feat)\n",
    "    power = librosa.power_to_db(mel_feat,ref=np.max)\n",
    "    # power = power.reshape(-1,1)\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_spectrograms(df, audio_root_dir):\n",
    "    video_paths = df[\"path\"].values.tolist()\n",
    "    video_ids = df[\"Video ID\"].values.tolist()\n",
    "    scene_ids = df[\"Scene_ID\"].values.tolist()\n",
    "\n",
    "    audio_features = []\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(path)\n",
    "        video_id = video_ids[idx]\n",
    "        scene_id = scene_ids[idx]\n",
    "\n",
    "        audio_dir = audio_root_dir + str(video_id) + '.0' + str(scene_id) + '.wav'\n",
    "        print(audio_dir)\n",
    "\n",
    "        mel = extract_mel_spectrograms(audio_dir)\n",
    "\n",
    "        audio_features.append(mel)\n",
    "\n",
    "    audio_features = np.array(audio_features)\n",
    "    print(audio_features.shape)\n",
    "    return audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(filename, audio_dir):\n",
    "    # plt.interactive(False)\n",
    "    clip, sample_rate = librosa.load(audio_dir, sr=None)\n",
    "    fig = plt.figure(figsize=[0.72,0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "    plt.show()\n",
    "    # plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "    # plt.close()    \n",
    "    # fig.clf()\n",
    "    # plt.close(fig)\n",
    "    # plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 51.84x51.84 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_spectrogram(\"test_spec.jpg\", \"test_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_spectrograms(df, audio_root_dir, save_dir):\n",
    "    video_paths = df[\"path\"].values.tolist()\n",
    "    video_ids = df[\"Video ID\"].values.tolist()\n",
    "    scene_ids = df[\"Scene_ID\"].values.tolist()\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(path)\n",
    "        video_id = video_ids[idx]\n",
    "        scene_id = scene_ids[idx]\n",
    "\n",
    "        audio_dir = audio_root_dir + str(video_id) + '.0' + str(scene_id) + '.wav'\n",
    "        print(audio_dir)\n",
    "\n",
    "        save_filename = save_dir + str(video_id) + '.0' + str(scene_id) + '.jpg'\n",
    "        print(save_filename)\n",
    "        create_spectrogram(save_filename, audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_train_path = os.getcwd() + \"/extracted_train_spectrogram/\"\n",
    "# extracted_val_path = os.getcwd() + \"/extracted_val_spectrogram/\"\n",
    "# extracted_test_path = os.getcwd() + \"/extracted_test_spectrogram/\"\n",
    "# # os.mkdir(extracted_train_path)\n",
    "# # os.mkdir(extracted_val_path)\n",
    "# # os.mkdir(extracted_test_path)\n",
    "\n",
    "# extract_all_spectrograms(train_df,train_audio_dir,extracted_train_path)\n",
    "# extract_all_spectrograms(val_df,val_audio_dir,extracted_val_path)\n",
    "# extract_all_spectrograms(test_df,test_audio_dir,extracted_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = np.load(\"extracted_audio_data/train_data.npy\", allow_pickle=True), np.load(\"extracted_data/train_labels.npy\")\n",
    "val_data, val_labels = np.load(\"extracted_audio_data/val_data.npy\", allow_pickle=True), np.load(\"extracted_data/val_labels.npy\")\n",
    "test_data, test_labels = np.load(\"extracted_audio_data/test_data.npy\", allow_pickle=True), np.load(\"extracted_data/test_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(942, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrograms = glob.glob('extracted_train_spectrogram/*')\n",
    "val_spectrograms = glob.glob('extracted_val_spectrogram/*')\n",
    "test_spectrograms = glob.glob('extracted_test_spectrogram/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for f in train_spectrograms:\n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    train_data.append(img)\n",
    "    \n",
    "train_data = np.array(train_data)\n",
    "\n",
    "for f in val_spectrograms:\n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    val_data.append(img)\n",
    "    \n",
    "val_data = np.array(val_data)\n",
    "\n",
    "for f in test_spectrograms:\n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    test_data.append(img)\n",
    "    \n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(942, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    classes = 4\n",
    "\n",
    "    # Create a VGG19 model, and removing the last layer that is classifying 1000 images. \n",
    "    # This will be replaced with images classes we have. \n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "    # freeze all layers in the the base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Model = Model(inputs=base_model.input, outputs=base_model.get_layer('flatten').output)\n",
    "\n",
    "    x = layers.Flatten()(base_model.output) #Output obtained on vgg16 is now flattened.\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    #Creating model object \n",
    "    model = keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    # optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    log_dir = \"logs/fit/temp\" \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    filepath = os.getcwd() + \"/temp/audio_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    # with tf.device('/device:CPU:0'):\n",
    "    model = get_cnn_model()\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        validation_data=(val_data,val_labels),\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint, tensorboard_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    # evaluate the model\n",
    "    # _, accuracy = model.evaluate(test_data, test_labels)\n",
    "   \n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "    print(f\"Precision: {round(precision, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 100356    \n",
      "=================================================================\n",
      "Total params: 20,124,740\n",
      "Trainable params: 100,356\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 5s 163ms/step - loss: 4.3644 - accuracy: 0.2951 - f1_m: 0.2645 - precision_m: 0.2669 - recall_m: 0.2867 - val_loss: 3.1271 - val_accuracy: 0.3717 - val_f1_m: 0.2273 - val_precision_m: 0.3331 - val_recall_m: 0.1797\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.12706, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/temp\\audio_classifier\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 3s 103ms/step - loss: 2.6711 - accuracy: 0.4087 - f1_m: 0.4139 - precision_m: 0.4289 - recall_m: 0.4174 - val_loss: 4.0595 - val_accuracy: 0.2124 - val_f1_m: 0.1363 - val_precision_m: 0.2321 - val_recall_m: 0.1038\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.12706\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 3s 102ms/step - loss: 1.9632 - accuracy: 0.4936 - f1_m: 0.5216 - precision_m: 0.5368 - recall_m: 0.5267 - val_loss: 3.2950 - val_accuracy: 0.2124 - val_f1_m: 0.1813 - val_precision_m: 0.2311 - val_recall_m: 0.1507\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.12706\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 3s 104ms/step - loss: 1.6785 - accuracy: 0.4788 - f1_m: 0.5371 - precision_m: 0.5576 - recall_m: 0.5427 - val_loss: 3.1530 - val_accuracy: 0.3274 - val_f1_m: 0.3357 - val_precision_m: 0.4109 - val_recall_m: 0.2969\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.12706\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 4s 125ms/step - loss: 1.2854 - accuracy: 0.5318 - f1_m: 0.6307 - precision_m: 0.6438 - recall_m: 0.6428 - val_loss: 3.6848 - val_accuracy: 0.3009 - val_f1_m: 0.2055 - val_precision_m: 0.2946 - val_recall_m: 0.1629\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.12706\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 1.1412 - accuracy: 0.5881 - f1_m: 0.6703 - precision_m: 0.6900 - recall_m: 0.6735 - val_loss: 3.6080 - val_accuracy: 0.2478 - val_f1_m: 0.2304 - val_precision_m: 0.3033 - val_recall_m: 0.1953\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.12706\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 3s 104ms/step - loss: 1.1668 - accuracy: 0.5456 - f1_m: 0.6667 - precision_m: 0.6771 - recall_m: 0.6694 - val_loss: 4.1330 - val_accuracy: 0.2301 - val_f1_m: 0.2033 - val_precision_m: 0.3136 - val_recall_m: 0.1540\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.12706\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 1.0865 - accuracy: 0.6030 - f1_m: 0.6885 - precision_m: 0.7045 - recall_m: 0.6939 - val_loss: 4.5983 - val_accuracy: 0.2566 - val_f1_m: 0.3105 - val_precision_m: 0.3510 - val_recall_m: 0.2913\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.12706\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.8500 - accuracy: 0.6200 - f1_m: 0.7223 - precision_m: 0.7316 - recall_m: 0.7248 - val_loss: 3.8633 - val_accuracy: 0.3009 - val_f1_m: 0.3574 - val_precision_m: 0.3885 - val_recall_m: 0.3393\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.12706\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.7805 - accuracy: 0.6338 - f1_m: 0.7352 - precision_m: 0.7529 - recall_m: 0.7336 - val_loss: 4.0950 - val_accuracy: 0.2655 - val_f1_m: 0.3110 - val_precision_m: 0.3528 - val_recall_m: 0.2824\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.12706\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 3s 106ms/step - loss: 0.6596 - accuracy: 0.6253 - f1_m: 0.7660 - precision_m: 0.7773 - recall_m: 0.7698 - val_loss: 4.6874 - val_accuracy: 0.3274 - val_f1_m: 0.1792 - val_precision_m: 0.2350 - val_recall_m: 0.1484\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 3.12706\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 3s 107ms/step - loss: 0.7353 - accuracy: 0.6146 - f1_m: 0.7637 - precision_m: 0.7832 - recall_m: 0.7716 - val_loss: 4.4022 - val_accuracy: 0.3363 - val_f1_m: 0.3022 - val_precision_m: 0.3351 - val_recall_m: 0.2801\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.12706\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 3s 104ms/step - loss: 0.5838 - accuracy: 0.6582 - f1_m: 0.8056 - precision_m: 0.8048 - recall_m: 0.8149 - val_loss: 4.4307 - val_accuracy: 0.4425 - val_f1_m: 0.2378 - val_precision_m: 0.3517 - val_recall_m: 0.1853\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.12706\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 3s 104ms/step - loss: 0.4771 - accuracy: 0.6550 - f1_m: 0.8311 - precision_m: 0.8381 - recall_m: 0.8320 - val_loss: 4.3361 - val_accuracy: 0.2920 - val_f1_m: 0.1856 - val_precision_m: 0.3056 - val_recall_m: 0.1373\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.12706\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 3s 104ms/step - loss: 0.4197 - accuracy: 0.6603 - f1_m: 0.8409 - precision_m: 0.8519 - recall_m: 0.8375 - val_loss: 3.9757 - val_accuracy: 0.3540 - val_f1_m: 0.2789 - val_precision_m: 0.3311 - val_recall_m: 0.2444\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.12706\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 3s 107ms/step - loss: 0.4785 - accuracy: 0.6805 - f1_m: 0.8346 - precision_m: 0.8387 - recall_m: 0.8410 - val_loss: 4.8890 - val_accuracy: 0.2035 - val_f1_m: 0.2219 - val_precision_m: 0.2731 - val_recall_m: 0.1953\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.12706\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 3s 104ms/step - loss: 0.6101 - accuracy: 0.6178 - f1_m: 0.8025 - precision_m: 0.8118 - recall_m: 0.8028 - val_loss: 4.6745 - val_accuracy: 0.2832 - val_f1_m: 0.2639 - val_precision_m: 0.3259 - val_recall_m: 0.2288\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.12706\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 3s 106ms/step - loss: 0.3895 - accuracy: 0.6858 - f1_m: 0.8567 - precision_m: 0.8610 - recall_m: 0.8573 - val_loss: 4.7466 - val_accuracy: 0.2566 - val_f1_m: 0.1544 - val_precision_m: 0.2667 - val_recall_m: 0.1127\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.12706\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 3s 106ms/step - loss: 0.4028 - accuracy: 0.6550 - f1_m: 0.8576 - precision_m: 0.8685 - recall_m: 0.8564 - val_loss: 5.0744 - val_accuracy: 0.3628 - val_f1_m: 0.1843 - val_precision_m: 0.2893 - val_recall_m: 0.1440\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.12706\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 3s 108ms/step - loss: 0.5703 - accuracy: 0.6709 - f1_m: 0.8268 - precision_m: 0.8359 - recall_m: 0.8267 - val_loss: 5.4435 - val_accuracy: 0.3097 - val_f1_m: 0.2946 - val_precision_m: 0.3674 - val_recall_m: 0.2589\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.12706\n",
      "Test accuracy: 28.97%\n",
      "F1 score: 0.3\n",
      "Precision: 0.33\n",
      "Recall: 0.32\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# from keras.preprocessing import image\n",
    "\n",
    "# Y_pred = []\n",
    "\n",
    "# for i in range(len(test_set)):\n",
    "#   img = image.load_img(path= test_set.Images[i],target_size=(256,256,3))\n",
    "#   img = image.img_to_array(img)\n",
    "#   test_img = img.reshape((1,256,256,3))\n",
    "#   img_class = classifier.predict_classes(test_img)\n",
    "#   prediction = img_class[0]\n",
    "#   Y_pred.append(prediction)\n",
    "\n",
    "img = cv2.imread('extracted_test_spectrogram/al-TxOuSqc8.02.jpg')\n",
    "img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "img = np.array(img)\n",
    "# img = test_data[104]\n",
    "img = img.reshape((1,IMG_SIZE,IMG_SIZE,3))\n",
    "# print(img.shape)\n",
    "\n",
    "y_pred = trained_model.predict(img)[0]\n",
    "\n",
    "# round probabilities to class labels\n",
    "y_pred = y_pred.round()\n",
    "\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(942, 150528)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "melspectrogram=train_data.reshape(942,-1)\n",
    "print(melspectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(melspectrogram)\n",
    "normalized_melspectrogram = scaler.transform(melspectrogram)\n",
    "\n",
    "print(np.amax(melspectrogram))\n",
    "print(np.amax(normalized_melspectrogram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_convolution = np.reshape(melspectrogram,(942,128, -1,1))\n",
    "#melspectrogram=melspectrogram.reshape(400,128, -1)\n",
    "features_convolution.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9abb609d9af7c865ec2d4837dadf5771495602565493ad3e886cad7ec3618278"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf.2.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
