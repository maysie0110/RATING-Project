{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from scipy.io import wavfile # scipy library to read wav files\n",
    "from scipy.fftpack import fft # fourier transform\n",
    "from scipy import signal\n",
    "\n",
    "import librosa.display\n",
    "import pylab\n",
    "import librosa    \n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities import f1_m, recall_m, precision_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "train_dir = os.getcwd() + \"/train_data/\"\n",
    "val_dir = os.getcwd() + \"/val_data/\"\n",
    "test_dir = os.getcwd() + \"/test_data/\"\n",
    "\n",
    "train_audio_dir = os.getcwd() + \"/extracted_train_audio/\"\n",
    "val_audio_dir = os.getcwd() + \"/extracted_val_audio/\"\n",
    "test_audio_dir = os.getcwd() + \"/extracted_test_audio/\"\n",
    "\n",
    "#directory to save extracted data\n",
    "extracted_data_dir = os.getcwd() + \"/extracted_audio_data/\"\n",
    "# os.mkdir(extracted_data_dir)\n",
    "\n",
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the training set.\n",
    "train_df = pd.read_csv('train-updated.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "train_df[\"path\"] = train_dir + train_df[\"Video ID\"]+ \".0\" + train_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "\n",
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the validation set.\n",
    "val_df = pd.read_csv('val.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "val_df[\"path\"] = val_dir + val_df[\"Video ID\"]+ \".0\" + val_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "\n",
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the test set.\n",
    "test_df = pd.read_csv('test-updated.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "test_df[\"path\"] = test_dir + test_df[\"Video ID\"]+ \".0\" + test_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrograms(audio_dir):\n",
    "    # Load the audio wav file into numpy array\n",
    "    audio_data, samplerate = librosa.load(audio_dir)\n",
    "    # print(audio_data.shape)\n",
    "    # print(samplerate)\n",
    "\n",
    "    #Compute a mel-scaled spectrogram\n",
    "    mel_feat = librosa.feature.melspectrogram(y=audio_data, sr=samplerate)\n",
    "    librosa.display.specshow(mel_feat)\n",
    "    power = librosa.power_to_db(mel_feat,ref=np.max)\n",
    "    # power = power.reshape(-1,1)\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 2613)\n"
     ]
    }
   ],
   "source": [
    "mel = extract_mel_spectrograms(\"my_result.wav\")\n",
    "print(mel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x1af247e8610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.display.specshow(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(audio_dir):\n",
    "    plt.interactive(False)\n",
    "    clip, sample_rate = librosa.load(audio_dir, sr=None)\n",
    "    fig = plt.figure(figsize=[0.72,0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "\n",
    "    filename  = 'my_result.jpg'\n",
    "    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "    plt.close()    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_spectrogram(\"my_result.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_spectrograms(df, audio_root_dir):\n",
    "    video_paths = df[\"path\"].values.tolist()\n",
    "    video_ids = df[\"Video ID\"].values.tolist()\n",
    "    scene_ids = df[\"Scene_ID\"].values.tolist()\n",
    "\n",
    "    audio_features = []\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(path)\n",
    "        video_id = video_ids[idx]\n",
    "        scene_id = scene_ids[idx]\n",
    "\n",
    "        audio_dir = audio_root_dir + str(video_id) + '.0' + str(scene_id) + '.wav'\n",
    "        print(audio_dir)\n",
    "\n",
    "        mel = extract_mel_spectrograms(audio_dir)\n",
    "\n",
    "        audio_features.append(mel)\n",
    "\n",
    "    audio_features = np.array(audio_features)\n",
    "    print(audio_features.shape)\n",
    "    return audio_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = np.load(\"extracted_audio_data/train_data.npy\", allow_pickle=True), np.load(\"extracted_data/train_labels.npy\")\n",
    "val_data, val_labels = np.load(\"extracted_audio_data/val_data.npy\", allow_pickle=True), np.load(\"extracted_data/val_labels.npy\")\n",
    "test_data, test_labels = np.load(\"extracted_audio_data/test_data.npy\", allow_pickle=True), np.load(\"extracted_data/test_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(942, 4)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrograms = glob.glob('extracted_train_spectrogram/*')\n",
    "val_spectrograms = glob.glob('extracted_val_spectrogram/*')\n",
    "test_spectrograms = glob.glob('extracted_test_spectrogram/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for f in train_spectrograms:\n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    train_data.append(img)\n",
    "    \n",
    "train_data = np.array(train_data)\n",
    "\n",
    "for f in val_spectrograms:\n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    val_data.append(img)\n",
    "    \n",
    "val_data = np.array(val_data)\n",
    "\n",
    "for f in test_spectrograms:\n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    test_data.append(img)\n",
    "    \n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(942, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    classes = 4\n",
    "\n",
    "    # Create a VGG19 model, and removing the last layer that is classifying 1000 images. \n",
    "    # # This will be replaced with images classes we have. \n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "    # freeze all layers in the the base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Model = Model(inputs=base_model.input, outputs=base_model.get_layer('flatten').output)\n",
    "\n",
    "    x = layers.Flatten()(base_model.output) #Output obtained on vgg16 is now flattened. \n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    #Creating model object \n",
    "    model = keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # compile the model\n",
    "    # model.compile(\n",
    "    #     optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    # )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    log_dir = \"logs/fit/temp\" \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    filepath = os.getcwd() + \"/temp/audio_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    # with tf.device('/device:CPU:0'):\n",
    "    model = get_cnn_model()\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        validation_data=(val_data,val_labels),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        callbacks=[checkpoint, tensorboard_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    # evaluate the model\n",
    "    _, accuracy = model.evaluate(test_data, test_labels)\n",
    "   \n",
    "    # loss, accuracy, f1_score, precision, recall = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    # print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "    # print(f\"Precision: {round(precision, 2)}\")\n",
    "    # print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4)                 100356    \n",
      "=================================================================\n",
      "Total params: 20,124,740\n",
      "Trainable params: 100,356\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "30/30 [==============================] - 5s 163ms/step - loss: 1.9759 - accuracy: 0.2866 - val_loss: 1.8106 - val_accuracy: 0.2920\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.81060, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/temp\\audio_classifier\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 1.2267 - accuracy: 0.3185 - val_loss: 1.7969 - val_accuracy: 0.3009\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.81060 to 1.79691, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/temp\\audio_classifier\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 4s 136ms/step - loss: 0.9675 - accuracy: 0.3620 - val_loss: 1.6340 - val_accuracy: 0.2478\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.79691 to 1.63400, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/temp\\audio_classifier\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 4s 143ms/step - loss: 0.7345 - accuracy: 0.4151 - val_loss: 1.5607 - val_accuracy: 0.2655\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.63400 to 1.56070, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/temp\\audio_classifier\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 4s 142ms/step - loss: 0.5638 - accuracy: 0.4607 - val_loss: 1.4949 - val_accuracy: 0.2832\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.56070 to 1.49493, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/temp\\audio_classifier\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 4s 146ms/step - loss: 0.4495 - accuracy: 0.4809 - val_loss: 1.5341 - val_accuracy: 0.2743\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.49493\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 4s 147ms/step - loss: 0.3781 - accuracy: 0.5499 - val_loss: 1.5666 - val_accuracy: 0.2301\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.49493\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 4s 139ms/step - loss: 0.3329 - accuracy: 0.5499 - val_loss: 1.4642 - val_accuracy: 0.2920\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.49493 to 1.46415, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/temp\\audio_classifier\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 4s 141ms/step - loss: 0.2429 - accuracy: 0.6040 - val_loss: 1.4878 - val_accuracy: 0.2920\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.46415\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 4s 139ms/step - loss: 0.2083 - accuracy: 0.6157 - val_loss: 1.5027 - val_accuracy: 0.2743\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.46415\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 4s 136ms/step - loss: 0.1943 - accuracy: 0.6327 - val_loss: 1.5230 - val_accuracy: 0.2301\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.46415\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 4s 138ms/step - loss: 0.1495 - accuracy: 0.6401 - val_loss: 1.4984 - val_accuracy: 0.2566\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.46415\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 0.1264 - accuracy: 0.6656 - val_loss: 1.4815 - val_accuracy: 0.2743\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.46415\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 4s 136ms/step - loss: 0.1079 - accuracy: 0.6720 - val_loss: 1.4914 - val_accuracy: 0.2389\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.46415\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 0.0902 - accuracy: 0.6805 - val_loss: 1.4837 - val_accuracy: 0.2389\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.46415\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 0.0782 - accuracy: 0.6741 - val_loss: 1.4895 - val_accuracy: 0.2478\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.46415\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 0.0742 - accuracy: 0.6741 - val_loss: 1.4948 - val_accuracy: 0.2655\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.46415\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 4s 138ms/step - loss: 0.0640 - accuracy: 0.6879 - val_loss: 1.4836 - val_accuracy: 0.2478\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.46415\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 0.0538 - accuracy: 0.6815 - val_loss: 1.5329 - val_accuracy: 0.2478\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.46415\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 0.0518 - accuracy: 0.6975 - val_loss: 1.5042 - val_accuracy: 0.2655\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.46415\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 1.2590 - accuracy: 0.2430\n",
      "Test accuracy: 24.3%\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# from keras.preprocessing import image\n",
    "\n",
    "# Y_pred = []\n",
    "\n",
    "# for i in range(len(test_set)):\n",
    "#   img = image.load_img(path= test_set.Images[i],target_size=(256,256,3))\n",
    "#   img = image.img_to_array(img)\n",
    "#   test_img = img.reshape((1,256,256,3))\n",
    "#   img_class = classifier.predict_classes(test_img)\n",
    "#   prediction = img_class[0]\n",
    "#   Y_pred.append(prediction)\n",
    "\n",
    "img = cv2.imread('extracted_test_spectrogram/al-TxOuSqc8.02.jpg')\n",
    "img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "img = np.array(img)\n",
    "# img = test_data[104]\n",
    "img = img.reshape((1,IMG_SIZE,IMG_SIZE,3))\n",
    "# print(img.shape)\n",
    "\n",
    "y_pred = trained_model.predict(img)[0]\n",
    "\n",
    "# round probabilities to class labels\n",
    "y_pred = y_pred.round()\n",
    "\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(942, 150528)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "melspectrogram=train_data.reshape(942,-1)\n",
    "print(melspectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(melspectrogram)\n",
    "normalized_melspectrogram = scaler.transform(melspectrogram)\n",
    "\n",
    "print(np.amax(melspectrogram))\n",
    "print(np.amax(normalized_melspectrogram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_convolution = np.reshape(melspectrogram,(942,128, -1,1))\n",
    "#melspectrogram=melspectrogram.reshape(400,128, -1)\n",
    "features_convolution.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9abb609d9af7c865ec2d4837dadf5771495602565493ad3e886cad7ec3618278"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf.2.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
