{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from scipy.io import wavfile # scipy library to read wav files\n",
    "from scipy.fftpack import fft # fourier transform\n",
    "from scipy import signal\n",
    "\n",
    "import librosa.display\n",
    "import pylab\n",
    "import librosa    \n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from utilities import f1_m, recall_m, precision_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "train_dir = os.getcwd() + \"/train_data/\"\n",
    "val_dir = os.getcwd() + \"/val_data/\"\n",
    "test_dir = os.getcwd() + \"/test_data/\"\n",
    "\n",
    "train_audio_dir = os.getcwd() + \"/extracted_train_audio/\"\n",
    "val_audio_dir = os.getcwd() + \"/extracted_val_audio/\"\n",
    "test_audio_dir = os.getcwd() + \"/extracted_test_audio/\"\n",
    "\n",
    "#directory to save extracted data\n",
    "extracted_data_dir = os.getcwd() + \"/extracted_audio_data/\"\n",
    "# os.mkdir(extracted_data_dir)\n",
    "\n",
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the training set.\n",
    "train_df = pd.read_csv('train-updated.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "train_df[\"path\"] = train_dir + train_df[\"Video ID\"]+ \".0\" + train_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "\n",
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the validation set.\n",
    "val_df = pd.read_csv('val.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "val_df[\"path\"] = val_dir + val_df[\"Video ID\"]+ \".0\" + val_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "\n",
    "# Create a dataframe which contains multiclass classification content annotations for each video scene used in the test set.\n",
    "test_df = pd.read_csv('test-updated.csv', dtype={'combination': object}).iloc[:,1:]\n",
    "test_df[\"path\"] = test_dir + test_df[\"Video ID\"]+ \".0\" + test_df[\"Scene_ID\"].astype(str) + \".mp4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.call([\"ffmpeg\", \"-y\", \"-i\", \"tt2872718.00.mp4\", \"test_audio.wav\"], \n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrograms(audio_dir):\n",
    "    # Load the audio wav file into numpy array\n",
    "    audio_data, samplerate = librosa.load(audio_dir)\n",
    "    # print(audio_data.shape)\n",
    "    # print(samplerate)\n",
    "\n",
    "    #Compute a mel-scaled spectrogram\n",
    "    mel_feat = librosa.feature.melspectrogram(y=audio_data, sr=samplerate)\n",
    "    librosa.display.specshow(mel_feat)\n",
    "    power = librosa.power_to_db(mel_feat,ref=np.max)\n",
    "    # power = power.reshape(-1,1)\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_spectrograms(df, audio_root_dir):\n",
    "    video_paths = df[\"path\"].values.tolist()\n",
    "    video_ids = df[\"Video ID\"].values.tolist()\n",
    "    scene_ids = df[\"Scene_ID\"].values.tolist()\n",
    "\n",
    "    audio_features = []\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(path)\n",
    "        video_id = video_ids[idx]\n",
    "        scene_id = scene_ids[idx]\n",
    "\n",
    "        audio_dir = audio_root_dir + str(video_id) + '.0' + str(scene_id) + '.wav'\n",
    "        print(audio_dir)\n",
    "\n",
    "        mel = extract_mel_spectrograms(audio_dir)\n",
    "\n",
    "        audio_features.append(mel)\n",
    "\n",
    "    audio_features = np.array(audio_features)\n",
    "    print(audio_features.shape)\n",
    "    return audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(filename, audio_dir):\n",
    "    # plt.interactive(False)\n",
    "    clip, sample_rate = librosa.load(audio_dir, sr=None)\n",
    "    fig = plt.figure(figsize=[0.72,0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "    plt.show()\n",
    "    # plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "    # plt.close()    \n",
    "    # fig.clf()\n",
    "    # plt.close(fig)\n",
    "    # plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_spectrogram(\"test_spec.jpg\", \"test_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_spectrograms(df, audio_root_dir, save_dir):\n",
    "    video_paths = df[\"path\"].values.tolist()\n",
    "    video_ids = df[\"Video ID\"].values.tolist()\n",
    "    scene_ids = df[\"Scene_ID\"].values.tolist()\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(path)\n",
    "        video_id = video_ids[idx]\n",
    "        scene_id = scene_ids[idx]\n",
    "\n",
    "        audio_dir = audio_root_dir + str(video_id) + '.0' + str(scene_id) + '.wav'\n",
    "        print(audio_dir)\n",
    "\n",
    "        save_filename = save_dir + str(video_id) + '.0' + str(scene_id) + '.jpg'\n",
    "        print(save_filename)\n",
    "        create_spectrogram(save_filename, audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_train_path = os.getcwd() + \"/extracted_train_spectrogram/\"\n",
    "# extracted_val_path = os.getcwd() + \"/extracted_val_spectrogram/\"\n",
    "# extracted_test_path = os.getcwd() + \"/extracted_test_spectrogram/\"\n",
    "# # os.mkdir(extracted_train_path)\n",
    "# # os.mkdir(extracted_val_path)\n",
    "# # os.mkdir(extracted_test_path)\n",
    "\n",
    "# extract_all_spectrograms(train_df,train_audio_dir,extracted_train_path)\n",
    "# extract_all_spectrograms(val_df,val_audio_dir,extracted_val_path)\n",
    "# extract_all_spectrograms(test_df,test_audio_dir,extracted_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = np.load(\"extracted_audio_data/train_data.npy\", allow_pickle=True), np.load(\"extracted_data/train_labels.npy\")\n",
    "val_data, val_labels = np.load(\"extracted_audio_data/val_data.npy\", allow_pickle=True), np.load(\"extracted_data/val_labels.npy\")\n",
    "test_data, test_labels = np.load(\"extracted_audio_data/test_data.npy\", allow_pickle=True), np.load(\"extracted_data/test_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrograms = glob.glob('extracted_train_spectrogram/*')\n",
    "val_spectrograms = glob.glob('extracted_val_spectrogram/*')\n",
    "test_spectrograms = glob.glob('extracted_test_spectrogram/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = []\n",
    "# val_data = []\n",
    "# test_data = []\n",
    "\n",
    "# for f in train_spectrograms:\n",
    "#     img = cv2.imread(f)\n",
    "#     img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "#     train_data.append(img)\n",
    "    \n",
    "# train_data = np.array(train_data)\n",
    "\n",
    "# for f in val_spectrograms:\n",
    "#     img = cv2.imread(f)\n",
    "#     img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "#     val_data.append(img)\n",
    "    \n",
    "# val_data = np.array(val_data)\n",
    "\n",
    "# for f in test_spectrograms:\n",
    "#     img = cv2.imread(f)\n",
    "#     img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "#     test_data.append(img)\n",
    "    \n",
    "# test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing import image\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for f in train_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    train_data.append(img)\n",
    "    \n",
    "train_data = np.array(train_data)\n",
    "print(train_data.shape)\n",
    "\n",
    "for f in val_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    val_data.append(img)\n",
    "    \n",
    "val_data = np.array(val_data)\n",
    "print(val_data.shape)\n",
    "\n",
    "for f in test_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    test_data.append(img)\n",
    "    \n",
    "test_data = np.array(test_data)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    # # Create a VGG19 model, and removing the last layer that is classifying 1000 images. \n",
    "    # # This will be replaced with images classes we have. \n",
    "    # base_model = VGG19(include_top=True, input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "    # # freeze all layers in the the base model\n",
    "    # # base_model.trainable = False\n",
    "\n",
    "    # # Model = Model(inputs=base_model.input, outputs=base_model.get_layer('flatten').output)\n",
    "\n",
    "    # # x = layers.Flatten()(base_model.output) #Output obtained on vgg19 is now flattened.\n",
    "    # x = base_model.output\n",
    "    # x = layers.Dense(4096, activation='relu')(x)\n",
    "    # outputs = layers.Dense(4, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # #Creating model object \n",
    "    # model = keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # model_pretrained = VGG19(include_top=True, input_shape=(IMG_SIZE, IMG_SIZE, 3))#, weights=\"imagenet\")\n",
    "    # x = layers.Dense(4096, activation='relu', name='predictions1', dtype='float32')(\n",
    "    #     model_pretrained.layers[-2].output)\n",
    "    # output = layers.Dense(4, activation='sigmoid', name='predictions', dtype='float32')(x)\n",
    "    # model = Model(model_pretrained.input, output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # compile the model\n",
    "    # optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # optimizer = keras.optimizers.SGD(learning_rate=10.0 ** (-1), decay=1e-5)\n",
    "    # optimizer = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n",
    "    optimizer = keras.optimizers.SGD(lr=0.0000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # optimizer = keras.optimizers.SGD(learning_rate=0.0005, decay=1e-6, momentum=0.9)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    log_dir = \"logs/fit/audio_chkpt\" \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    filepath = os.getcwd() + \"/audio_chkpt/audio_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True,\n",
    "        monitor='val_f1_m',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "    with tf.device('/device:CPU:0'):\n",
    "        model = get_cnn_model()\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            train_labels,\n",
    "            validation_data=(val_data,val_labels),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint, tensorboard_callback]\n",
    "        )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    # evaluate the model\n",
    "    # _, accuracy = model.evaluate(test_data, test_labels)\n",
    "   \n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "    print(f\"Precision: {round(precision, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_2 = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = trained_model.predict(test_data)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "# print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_pred))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing import image\n",
    "\n",
    "# Y_pred = []\n",
    "\n",
    "# for i in range(len(test_set)):\n",
    "#   img = image.load_img(path= test_set.Images[i],target_size=(256,256,3))\n",
    "#   img = image.img_to_array(img)\n",
    "#   test_img = img.reshape((1,256,256,3))\n",
    "#   img_class = classifier.predict_classes(test_img)\n",
    "#   prediction = img_class[0]\n",
    "#   Y_pred.append(prediction)\n",
    "\n",
    "img = cv2.imread('extracted_test_spectrogram/al-TxOuSqc8.02.jpg')\n",
    "img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "img = np.array(img)\n",
    "# img = test_data[104]\n",
    "img = img.reshape((1,IMG_SIZE,IMG_SIZE,3))\n",
    "# print(img.shape)\n",
    "\n",
    "y_pred = trained_model.predict(img)[0]\n",
    "\n",
    "# # round probabilities to class labels\n",
    "# y_pred = y_pred.round()\n",
    "\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "melspectrogram=train_data.reshape(942,-1)\n",
    "print(melspectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(melspectrogram)\n",
    "normalized_melspectrogram = scaler.transform(melspectrogram)\n",
    "\n",
    "print(np.amax(melspectrogram))\n",
    "print(np.amax(normalized_melspectrogram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_convolution = np.reshape(melspectrogram,(942,128, -1,1))\n",
    "#melspectrogram=melspectrogram.reshape(400,128, -1)\n",
    "features_convolution.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9abb609d9af7c865ec2d4837dadf5771495602565493ad3e886cad7ec3618278"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf.2.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
