{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from utilities import f1_m, recall_m, precision_m\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH = 128\n",
    "FRAME_GAP = 11\n",
    "NUM_FEATURES = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    classes = 4\n",
    "\n",
    "    # Create a VGG19 model, and removing the last layer that is classifying 1000 images. \n",
    "    # # This will be replaced with images classes we have. \n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "    # freeze all layers in the the base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Model = Model(inputs=base_model.input, outputs=base_model.get_layer('flatten').output)\n",
    "\n",
    "    x = layers.Flatten()(base_model.output) #Output obtained on vgg16 is now flattened. \n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    #Creating model object \n",
    "    model = keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"output_dim\": self.output_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# Subclassed layer\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = 4\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None), name=\"input\")\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "\n",
    "    x = layers.Dense(units=embed_dim, activation='gelu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 100356    \n",
      "=================================================================\n",
      "Total params: 20,124,740\n",
      "Trainable params: 100,356\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x145652db400>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filepath = os.getcwd() + \"/temp/audio_classifier\"\n",
    "cnn = get_cnn_model()\n",
    "cnn.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, None)]      0         \n",
      "_________________________________________________________________\n",
      "frame_position_embedding (Po (None, None, 1024)        131072    \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 1024)        4211716   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, None, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, None, 1024)        2048      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 5,398,536\n",
      "Trainable params: 5,398,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x145653a4a90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filepath = os.getcwd() + \"/tmp_3_4/video_classifier\"\n",
    "transformer = get_transformer_model()\n",
    "transformer.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_late_fusion_model(model_1,model_2):\n",
    "    classes = 4\n",
    "    x1 = model_1.output\n",
    "    x2 = model_2.output\n",
    "    \n",
    "    # LATE FUSION\n",
    "    x = layers.concatenate([x1, x2])\n",
    "    x = keras.Sequential()(x)\n",
    "    # x = Dense(x.shape[1], activation='relu')(x) #12\n",
    "    # x = Dropout(DROPOUT_PROB)(x)\n",
    "    # x = Dense(ceil(x.shape[1]/2), activation='relu')(x) #8\n",
    "    # x = Dropout(DROPOUT_PROB)(x)\n",
    "    # predictions = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    # x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=[model_1.input, model_2.input], outputs=outputs) # Inputs go into two different layers\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multimodal video classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_data, train_labels = np.load(\"extracted_data/train_data.npy\"), np.load(\"extracted_data/train_labels.npy\")\n",
    "val_image_data, val_labels = np.load(\"extracted_data/val_data.npy\"), np.load(\"extracted_data/val_labels.npy\")\n",
    "test_image_data, test_labels = np.load(\"extracted_data/test_data.npy\"), np.load(\"extracted_data/test_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrograms = glob.glob('extracted_train_spectrogram/*')\n",
    "val_spectrograms = glob.glob('extracted_val_spectrogram/*')\n",
    "test_spectrograms = glob.glob('extracted_test_spectrogram/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_data = []\n",
    "val_audio_data = []\n",
    "test_audio_data = []\n",
    "\n",
    "for f in train_spectrograms:\n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    train_audio_data.append(img)\n",
    "    \n",
    "train_audio_data = np.array(train_audio_data)\n",
    "\n",
    "for f in val_spectrograms:\n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    val_audio_data.append(img)\n",
    "    \n",
    "val_audio_data = np.array(val_audio_data)\n",
    "\n",
    "for f in test_spectrograms:\n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    test_audio_data.append(img)\n",
    "    \n",
    "test_audio_data = np.array(test_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    log_dir = \"logs/fit/fusion_temp\" \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    filepath = os.getcwd() + \"/fusion_temp/classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    with tf.device('/device:CPU:0'):\n",
    "        model = get_late_fusion_model(transformer,cnn)\n",
    "        history = model.fit(\n",
    "            [train_image_data, train_audio_data],\n",
    "            train_labels,\n",
    "            validation_data=([val_image_data, val_audio_data],val_labels),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint, tensorboard_callback],\n",
    "        )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    # _, accuracy = model.evaluate(test_data, test_labels)\n",
    "    # evaluate the model\n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate([test_image_data, test_audio_data], test_labels, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "    print(f\"Precision: {round(precision, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv4 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv4 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input (InputLayer)              [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "frame_position_embedding (Posit (None, None, 1024)   131072      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "transformer_layer (TransformerE (None, None, 1024)   4211716     frame_position_embedding[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, None, 1024)   1049600     transformer_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, None, 1024)   2048        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv4 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 1024)         0           layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 25088)        0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4)            4100        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            100356      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8)            0           dense_9[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       multiple             0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 8)            0           sequential_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 4)            36          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 25,523,312\n",
      "Trainable params: 5,498,928\n",
      "Non-trainable params: 20,024,384\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 99s 3s/step - loss: 0.6186 - accuracy: 0.2442 - f1_m: 0.2304 - precision_m: 0.2751 - recall_m: 0.2150 - val_loss: 0.6048 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60475, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 98s 3s/step - loss: 0.6091 - accuracy: 0.2951 - f1_m: 0.1941 - precision_m: 0.2649 - recall_m: 0.1605 - val_loss: 0.6035 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60475 to 0.60349, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 101s 3s/step - loss: 0.6114 - accuracy: 0.2909 - f1_m: 0.1877 - precision_m: 0.2624 - recall_m: 0.1511 - val_loss: 0.6023 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.60349 to 0.60229, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 101s 3s/step - loss: 0.6045 - accuracy: 0.2803 - f1_m: 0.2091 - precision_m: 0.2818 - recall_m: 0.1714 - val_loss: 0.6011 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60229 to 0.60113, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 104s 3s/step - loss: 0.6070 - accuracy: 0.2749 - f1_m: 0.1787 - precision_m: 0.2379 - recall_m: 0.1483 - val_loss: 0.6000 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.60113 to 0.59995, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 103s 3s/step - loss: 0.6079 - accuracy: 0.3163 - f1_m: 0.2176 - precision_m: 0.3037 - recall_m: 0.1757 - val_loss: 0.5988 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.59995 to 0.59876, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 103s 3s/step - loss: 0.6083 - accuracy: 0.2951 - f1_m: 0.1802 - precision_m: 0.2694 - recall_m: 0.1428 - val_loss: 0.5977 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.59876 to 0.59767, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 103s 3s/step - loss: 0.6058 - accuracy: 0.3047 - f1_m: 0.2102 - precision_m: 0.2885 - recall_m: 0.1736 - val_loss: 0.5966 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.59767 to 0.59660, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 103s 3s/step - loss: 0.6055 - accuracy: 0.2877 - f1_m: 0.2006 - precision_m: 0.2848 - recall_m: 0.1624 - val_loss: 0.5955 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.59660 to 0.59549, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 103s 3s/step - loss: 0.6100 - accuracy: 0.3036 - f1_m: 0.1796 - precision_m: 0.2448 - recall_m: 0.1483 - val_loss: 0.5945 - val_accuracy: 0.2301 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.59549 to 0.59445, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/fusion_temp\\classifier\n",
      "Test accuracy: 30.84%\n",
      "F1 score: 0.01\n",
      "Precision: 0.25\n",
      "Recall: 0.01\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9abb609d9af7c865ec2d4837dadf5771495602565493ad3e886cad7ec3618278"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf.2.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
