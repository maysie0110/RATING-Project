{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from utilities import f1_m, recall_m, precision_m\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH = 128\n",
    "FRAME_GAP = 11\n",
    "NUM_FEATURES = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "\n",
    "    #Create CNN model\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    # # compile the model\n",
    "    # optimizer = keras.optimizers.SGD(learning_rate=0.0000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # model.compile(\n",
    "    #     optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    # )\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"output_dim\": self.output_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# Subclassed layer\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = 4\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None), name=\"input\")\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "\n",
    "    x = layers.Dense(units=embed_dim, activation='gelu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "    # optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # # compile the model\n",
    "    # model.compile(\n",
    "    #     optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    # )\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_early_fusion_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = 4\n",
    "\n",
    "    # Create Transformer-based model\n",
    "    inputs_rgb = keras.Input(shape=(None, None), name=\"input_image\")\n",
    "    x1 = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs_rgb)\n",
    "    x1 = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x1)\n",
    "    x1 = layers.Dense(units=embed_dim, activation='gelu')(x1)\n",
    "    x1 = layers.LayerNormalization()(x1)\n",
    "    x1 = layers.GlobalMaxPooling1D()(x1)\n",
    "    x1 = layers.Dropout(0.5)(x1)\n",
    "\n",
    "\n",
    "    #Create CNN model\n",
    "    inputs_spec = keras.Input(shape=(IMG_SIZE,IMG_SIZE,3), name=\"input_spectrogram\")\n",
    "    # x2 = keras.Sequential()(inputs_spec)\n",
    "    x2 = layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3))(inputs_spec)\n",
    "    x2 = layers.Conv2D(64, (3, 3), activation='relu')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "    x2 = layers.Dropout(0.25)(x2)\n",
    "    x2 = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x2)\n",
    "    x2 = layers.Conv2D(64, (3, 3), activation='relu')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "    x2 = layers.Dropout(0.5)(x2)\n",
    "    x2 = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x2)\n",
    "    x2 = layers.Conv2D(128, (3, 3), activation='relu')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "    x2 = layers.Dropout(0.5)(x2)\n",
    "    x2 = layers.Flatten()(x2)\n",
    "    x2 = layers.Dense(512, activation='relu')(x2)\n",
    "    x2 = layers.Dropout(0.5)(x2)\n",
    "\n",
    "    # LATE FUSION\n",
    "    x = layers.concatenate([x1, x2])\n",
    "    x = keras.Sequential()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "    \n",
    "    model = keras.Model(inputs=[inputs_rgb, inputs_spec], outputs=outputs) # Inputs go into two different layers\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_early_fusion_model(model_1,model_2):\n",
    "#     x1 = model_1.output\n",
    "#     x2 = model_2.output\n",
    "#     classes = 4\n",
    "\n",
    "#     # LATE FUSION\n",
    "#     x = layers.concatenate([x1, x2])\n",
    "#     x = keras.Sequential()(x)\n",
    "#     x = layers.Dropout(0.5)(x)\n",
    "#     outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "#     model = keras.Model(inputs=[model_1.input, model_2.input], outputs=outputs) # Inputs go into two different layers\n",
    "\n",
    "#     optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "#     # compile the model\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "#     )\n",
    "    \n",
    "#     model.summary()\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train multimodal video classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_data, train_labels = np.load(\"extracted_data/train_data.npy\"), np.load(\"extracted_data/train_labels.npy\")\n",
    "val_image_data, val_labels = np.load(\"extracted_data/val_data.npy\"), np.load(\"extracted_data/val_labels.npy\")\n",
    "test_image_data, test_labels = np.load(\"extracted_data/test_data.npy\"), np.load(\"extracted_data/test_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrograms = glob.glob('extracted_train_spectrogram/*')\n",
    "val_spectrograms = glob.glob('extracted_val_spectrogram/*')\n",
    "test_spectrograms = glob.glob('extracted_test_spectrogram/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing import image\n",
    "train_audio_data = []\n",
    "val_audio_data = []\n",
    "test_audio_data = []\n",
    "\n",
    "for f in train_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    train_audio_data.append(img)\n",
    "    \n",
    "train_audio_data = np.array(train_audio_data)\n",
    "\n",
    "for f in val_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    val_audio_data.append(img)\n",
    "    \n",
    "val_audio_data = np.array(val_audio_data)\n",
    "\n",
    "for f in test_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    test_audio_data.append(img)\n",
    "    \n",
    "test_audio_data = np.array(test_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, None)]      0         \n",
      "_________________________________________________________________\n",
      "frame_position_embedding (Po (None, None, 1024)        131072    \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 1024)        4211716   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (None, None, 1024)        2048      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 5,398,536\n",
      "Trainable params: 5,398,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 111, 111, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 111, 111, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 111, 111, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 109, 109, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 54, 54, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 52, 52, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               44302848  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 44,619,588\n",
      "Trainable params: 44,619,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer = get_transformer_model()\n",
    "cnn = get_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    log_dir = \"logs/fit/fusion_temp\" \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    filepath = os.getcwd() + \"/early_fusion_temp/classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, monitor='val_f1_m',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "    with tf.device('/device:CPU:0'):\n",
    "        # model = get_early_fusion_model(transformer,cnn)\n",
    "        model = get_early_fusion_model()\n",
    "        history = model.fit(\n",
    "            [train_image_data, train_audio_data],\n",
    "            train_labels,\n",
    "            validation_data=([val_image_data, val_audio_data],val_labels),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint, tensorboard_callback],\n",
    "        )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    # _, accuracy = model.evaluate(test_data, test_labels)\n",
    "    # evaluate the model\n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate([test_image_data, test_audio_data], test_labels, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "    print(f\"Precision: {round(precision, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_spectrogram (InputLayer)  [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 224, 224, 32) 896         input_spectrogram[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 222, 222, 64) 18496       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 111, 111, 64) 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 111, 111, 64) 0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 111, 111, 64) 36928       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 109, 109, 64) 36928       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 54, 54, 64)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 54, 54, 64)   0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_image (InputLayer)        [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 54, 54, 128)  73856       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "frame_position_embedding (Posit (None, None, 1024)   131072      input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 52, 52, 128)  147584      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "transformer_layer (TransformerE (None, None, 1024)   4211716     frame_position_embedding[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 26, 26, 128)  0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, None, 1024)   1049600     transformer_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 26, 26, 128)  0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, None, 1024)   2048        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 86528)        0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 1024)         0           layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          44302848    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1536)         0           dropout_5[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       multiple             0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1536)         0           sequential_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 4)            6148        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 50,018,120\n",
      "Trainable params: 50,018,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "30/30 [==============================] - 134s 4s/step - loss: 7.1851 - accuracy: 0.3025 - f1_m: 0.2636 - precision_m: 0.2575 - recall_m: 0.2880 - val_loss: 0.7033 - val_accuracy: 0.2301 - val_f1_m: 0.3655 - val_precision_m: 0.4196 - val_recall_m: 0.3438\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.36548, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp\\classifier\n",
      "Epoch 2/30\n",
      "30/30 [==============================] - 127s 4s/step - loss: 0.9105 - accuracy: 0.3577 - f1_m: 0.3303 - precision_m: 0.3515 - recall_m: 0.3211 - val_loss: 0.5675 - val_accuracy: 0.3186 - val_f1_m: 0.1815 - val_precision_m: 0.3552 - val_recall_m: 0.1250\n",
      "\n",
      "Epoch 00002: val_f1_m did not improve from 0.36548\n",
      "Epoch 3/30\n",
      "30/30 [==============================] - 126s 4s/step - loss: 0.7252 - accuracy: 0.3875 - f1_m: 0.3635 - precision_m: 0.4036 - recall_m: 0.3406 - val_loss: 0.5171 - val_accuracy: 0.4248 - val_f1_m: 0.1117 - val_precision_m: 0.3750 - val_recall_m: 0.0658\n",
      "\n",
      "Epoch 00003: val_f1_m did not improve from 0.36548\n",
      "Epoch 4/30\n",
      "30/30 [==============================] - 127s 4s/step - loss: 0.6342 - accuracy: 0.4098 - f1_m: 0.3536 - precision_m: 0.4140 - recall_m: 0.3153 - val_loss: 0.5132 - val_accuracy: 0.3982 - val_f1_m: 0.2635 - val_precision_m: 0.5056 - val_recall_m: 0.1842\n",
      "\n",
      "Epoch 00004: val_f1_m did not improve from 0.36548\n",
      "Epoch 5/30\n",
      "30/30 [==============================] - 124s 4s/step - loss: 0.6003 - accuracy: 0.3811 - f1_m: 0.3473 - precision_m: 0.4133 - recall_m: 0.3077 - val_loss: 0.5170 - val_accuracy: 0.3982 - val_f1_m: 0.2620 - val_precision_m: 0.5156 - val_recall_m: 0.1842\n",
      "\n",
      "Epoch 00005: val_f1_m did not improve from 0.36548\n",
      "Epoch 6/30\n",
      "30/30 [==============================] - 129s 4s/step - loss: 0.5250 - accuracy: 0.4225 - f1_m: 0.4035 - precision_m: 0.4863 - recall_m: 0.3510 - val_loss: 0.4721 - val_accuracy: 0.4248 - val_f1_m: 0.2728 - val_precision_m: 0.5315 - val_recall_m: 0.1920\n",
      "\n",
      "Epoch 00006: val_f1_m did not improve from 0.36548\n",
      "Epoch 7/30\n",
      "30/30 [==============================] - 125s 4s/step - loss: 0.4921 - accuracy: 0.4299 - f1_m: 0.4224 - precision_m: 0.5240 - recall_m: 0.3612 - val_loss: 0.4611 - val_accuracy: 0.4425 - val_f1_m: 0.2297 - val_precision_m: 0.4025 - val_recall_m: 0.1629\n",
      "\n",
      "Epoch 00007: val_f1_m did not improve from 0.36548\n",
      "Epoch 8/30\n",
      "30/30 [==============================] - 127s 4s/step - loss: 0.4803 - accuracy: 0.4310 - f1_m: 0.4455 - precision_m: 0.5236 - recall_m: 0.3946 - val_loss: 0.4790 - val_accuracy: 0.4248 - val_f1_m: 0.1964 - val_precision_m: 0.4196 - val_recall_m: 0.1317\n",
      "\n",
      "Epoch 00008: val_f1_m did not improve from 0.36548\n",
      "Epoch 9/30\n",
      "30/30 [==============================] - 125s 4s/step - loss: 0.4633 - accuracy: 0.4374 - f1_m: 0.4660 - precision_m: 0.5473 - recall_m: 0.4118 - val_loss: 0.4814 - val_accuracy: 0.4071 - val_f1_m: 0.2421 - val_precision_m: 0.5204 - val_recall_m: 0.1674\n",
      "\n",
      "Epoch 00009: val_f1_m did not improve from 0.36548\n",
      "Epoch 10/30\n",
      "30/30 [==============================] - 125s 4s/step - loss: 0.4282 - accuracy: 0.5042 - f1_m: 0.4986 - precision_m: 0.5918 - recall_m: 0.4407 - val_loss: 0.4842 - val_accuracy: 0.4071 - val_f1_m: 0.1981 - val_precision_m: 0.4375 - val_recall_m: 0.1328\n",
      "\n",
      "Epoch 00010: val_f1_m did not improve from 0.36548\n",
      "Epoch 11/30\n",
      "30/30 [==============================] - 127s 4s/step - loss: 0.4140 - accuracy: 0.4862 - f1_m: 0.5384 - precision_m: 0.6305 - recall_m: 0.4835 - val_loss: 0.4680 - val_accuracy: 0.4336 - val_f1_m: 0.3009 - val_precision_m: 0.5649 - val_recall_m: 0.2277\n",
      "\n",
      "Epoch 00011: val_f1_m did not improve from 0.36548\n",
      "Epoch 12/30\n",
      "30/30 [==============================] - 127s 4s/step - loss: 0.3946 - accuracy: 0.5149 - f1_m: 0.5282 - precision_m: 0.6233 - recall_m: 0.4683 - val_loss: 0.4659 - val_accuracy: 0.3894 - val_f1_m: 0.3230 - val_precision_m: 0.5410 - val_recall_m: 0.2511\n",
      "\n",
      "Epoch 00012: val_f1_m did not improve from 0.36548\n",
      "Epoch 13/30\n",
      "30/30 [==============================] - 127s 4s/step - loss: 0.3711 - accuracy: 0.5127 - f1_m: 0.5595 - precision_m: 0.6438 - recall_m: 0.5065 - val_loss: 0.4603 - val_accuracy: 0.4159 - val_f1_m: 0.3480 - val_precision_m: 0.5975 - val_recall_m: 0.2656\n",
      "\n",
      "Epoch 00013: val_f1_m did not improve from 0.36548\n",
      "Epoch 14/30\n",
      "30/30 [==============================] - 125s 4s/step - loss: 0.3699 - accuracy: 0.5265 - f1_m: 0.5582 - precision_m: 0.6603 - recall_m: 0.4927 - val_loss: 0.4678 - val_accuracy: 0.3628 - val_f1_m: 0.3732 - val_precision_m: 0.5969 - val_recall_m: 0.2935\n",
      "\n",
      "Epoch 00014: val_f1_m improved from 0.36548 to 0.37318, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp\\classifier\n",
      "Epoch 15/30\n",
      "30/30 [==============================] - 125s 4s/step - loss: 0.3631 - accuracy: 0.5096 - f1_m: 0.5819 - precision_m: 0.6678 - recall_m: 0.5244 - val_loss: 0.4852 - val_accuracy: 0.4248 - val_f1_m: 0.2593 - val_precision_m: 0.5216 - val_recall_m: 0.1830\n",
      "\n",
      "Epoch 00015: val_f1_m did not improve from 0.37318\n",
      "Epoch 16/30\n",
      "30/30 [==============================] - 130s 4s/step - loss: 0.3462 - accuracy: 0.5244 - f1_m: 0.5952 - precision_m: 0.6854 - recall_m: 0.5387 - val_loss: 0.4770 - val_accuracy: 0.3894 - val_f1_m: 0.3840 - val_precision_m: 0.6625 - val_recall_m: 0.2846\n",
      "\n",
      "Epoch 00016: val_f1_m improved from 0.37318 to 0.38396, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp\\classifier\n",
      "Epoch 17/30\n",
      "30/30 [==============================] - 132s 4s/step - loss: 0.3319 - accuracy: 0.5393 - f1_m: 0.6108 - precision_m: 0.7131 - recall_m: 0.5425 - val_loss: 0.5153 - val_accuracy: 0.3451 - val_f1_m: 0.3199 - val_precision_m: 0.5876 - val_recall_m: 0.2321\n",
      "\n",
      "Epoch 00017: val_f1_m did not improve from 0.38396\n",
      "Epoch 18/30\n",
      "30/30 [==============================] - 128s 4s/step - loss: 0.3254 - accuracy: 0.5403 - f1_m: 0.6475 - precision_m: 0.7226 - recall_m: 0.5962 - val_loss: 0.5052 - val_accuracy: 0.3805 - val_f1_m: 0.3467 - val_precision_m: 0.6253 - val_recall_m: 0.2522\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.38396\n",
      "Epoch 19/30\n",
      "30/30 [==============================] - 127s 4s/step - loss: 0.3140 - accuracy: 0.5467 - f1_m: 0.6256 - precision_m: 0.7275 - recall_m: 0.5581 - val_loss: 0.5184 - val_accuracy: 0.3894 - val_f1_m: 0.3226 - val_precision_m: 0.5278 - val_recall_m: 0.2411\n",
      "\n",
      "Epoch 00019: val_f1_m did not improve from 0.38396\n",
      "Epoch 20/30\n",
      "30/30 [==============================] - 127s 4s/step - loss: 0.2990 - accuracy: 0.5563 - f1_m: 0.6596 - precision_m: 0.7576 - recall_m: 0.5954 - val_loss: 0.5346 - val_accuracy: 0.3894 - val_f1_m: 0.3493 - val_precision_m: 0.5108 - val_recall_m: 0.2723\n",
      "\n",
      "Epoch 00020: val_f1_m did not improve from 0.38396\n",
      "Epoch 21/30\n",
      "30/30 [==============================] - 107s 4s/step - loss: 0.2971 - accuracy: 0.5637 - f1_m: 0.6868 - precision_m: 0.7543 - recall_m: 0.6412 - val_loss: 0.5269 - val_accuracy: 0.4336 - val_f1_m: 0.3810 - val_precision_m: 0.5986 - val_recall_m: 0.2879\n",
      "\n",
      "Epoch 00021: val_f1_m did not improve from 0.38396\n",
      "Epoch 22/30\n",
      "30/30 [==============================] - 102s 3s/step - loss: 0.2779 - accuracy: 0.5722 - f1_m: 0.6968 - precision_m: 0.7650 - recall_m: 0.6487 - val_loss: 0.5949 - val_accuracy: 0.3451 - val_f1_m: 0.2920 - val_precision_m: 0.5959 - val_recall_m: 0.1987\n",
      "\n",
      "Epoch 00022: val_f1_m did not improve from 0.38396\n",
      "Epoch 23/30\n",
      "30/30 [==============================] - 101s 3s/step - loss: 0.2776 - accuracy: 0.5637 - f1_m: 0.7191 - precision_m: 0.7940 - recall_m: 0.6674 - val_loss: 0.5707 - val_accuracy: 0.3451 - val_f1_m: 0.3742 - val_precision_m: 0.5737 - val_recall_m: 0.2857\n",
      "\n",
      "Epoch 00023: val_f1_m did not improve from 0.38396\n",
      "Epoch 24/30\n",
      "30/30 [==============================] - 102s 3s/step - loss: 0.2622 - accuracy: 0.5870 - f1_m: 0.7316 - precision_m: 0.7893 - recall_m: 0.6923 - val_loss: 0.6084 - val_accuracy: 0.3451 - val_f1_m: 0.3698 - val_precision_m: 0.5981 - val_recall_m: 0.2734\n",
      "\n",
      "Epoch 00024: val_f1_m did not improve from 0.38396\n",
      "Epoch 25/30\n",
      "30/30 [==============================] - 102s 3s/step - loss: 0.2356 - accuracy: 0.5955 - f1_m: 0.7495 - precision_m: 0.8143 - recall_m: 0.6975 - val_loss: 0.6122 - val_accuracy: 0.3628 - val_f1_m: 0.3950 - val_precision_m: 0.5771 - val_recall_m: 0.3058\n",
      "\n",
      "Epoch 00025: val_f1_m improved from 0.38396 to 0.39501, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp\\classifier\n",
      "Epoch 26/30\n",
      "30/30 [==============================] - 105s 4s/step - loss: 0.2408 - accuracy: 0.5892 - f1_m: 0.7469 - precision_m: 0.8084 - recall_m: 0.7029 - val_loss: 0.5800 - val_accuracy: 0.3982 - val_f1_m: 0.4391 - val_precision_m: 0.5436 - val_recall_m: 0.3772\n",
      "\n",
      "Epoch 00026: val_f1_m improved from 0.39501 to 0.43912, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp\\classifier\n",
      "Epoch 27/30\n",
      "30/30 [==============================] - 101s 3s/step - loss: 0.2323 - accuracy: 0.6040 - f1_m: 0.7726 - precision_m: 0.8334 - recall_m: 0.7341 - val_loss: 0.6025 - val_accuracy: 0.3805 - val_f1_m: 0.4340 - val_precision_m: 0.6224 - val_recall_m: 0.3449\n",
      "\n",
      "Epoch 00027: val_f1_m did not improve from 0.43912\n",
      "Epoch 28/30\n",
      "30/30 [==============================] - 107s 4s/step - loss: 0.2127 - accuracy: 0.5998 - f1_m: 0.7826 - precision_m: 0.8312 - recall_m: 0.7466 - val_loss: 0.7100 - val_accuracy: 0.3540 - val_f1_m: 0.4446 - val_precision_m: 0.5659 - val_recall_m: 0.3717\n",
      "\n",
      "Epoch 00028: val_f1_m improved from 0.43912 to 0.44461, saving model to c:\\Users\\maitr\\Desktop\\COSC6373-ComputerVision\\Rating\\RATING-Project\\RATING-Project/early_fusion_temp\\classifier\n",
      "Epoch 29/30\n",
      "30/30 [==============================] - 121s 4s/step - loss: 0.2136 - accuracy: 0.6093 - f1_m: 0.7889 - precision_m: 0.8348 - recall_m: 0.7563 - val_loss: 0.7246 - val_accuracy: 0.3186 - val_f1_m: 0.4082 - val_precision_m: 0.5654 - val_recall_m: 0.3248\n",
      "\n",
      "Epoch 00029: val_f1_m did not improve from 0.44461\n",
      "Epoch 30/30\n",
      "30/30 [==============================] - 122s 4s/step - loss: 0.1972 - accuracy: 0.6062 - f1_m: 0.8080 - precision_m: 0.8452 - recall_m: 0.7782 - val_loss: 0.7189 - val_accuracy: 0.3363 - val_f1_m: 0.3561 - val_precision_m: 0.5538 - val_recall_m: 0.2667\n",
      "\n",
      "Epoch 00030: val_f1_m did not improve from 0.44461\n",
      "Test accuracy: 49.53%\n",
      "F1 score: 0.56\n",
      "Precision: 0.6\n",
      "Recall: 0.55\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "\n",
    "    #Create CNN model\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    # compile the model\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.0000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_transformer_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = 4\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None), name=\"input\")\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "\n",
    "    x = layers.Dense(units=embed_dim, activation='gelu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, None)]      0         \n",
      "_________________________________________________________________\n",
      "frame_position_embedding (Po (None, None, 1024)        131072    \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 1024)        4211716   \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, None, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "layer_normalization_16 (Laye (None, None, 1024)        2048      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 5,398,536\n",
      "Trainable params: 5,398,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test accuracy: 51.4%\n",
      "F1 score: 0.57\n",
      "Precision: 0.56\n",
      "Recall: 0.59\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 222, 222, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 111, 111, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 111, 111, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 111, 111, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 109, 109, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 54, 54, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 52, 52, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 512)               44302848  \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 44,619,588\n",
      "Trainable params: 44,619,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test accuracy: 50.47%\n",
      "F1 score: 0.33\n",
      "Precision: 0.24\n",
      "Recall: 0.57\n"
     ]
    }
   ],
   "source": [
    "# filepath = os.getcwd() + \"/tmp_3_4/video_classifier\"\n",
    "filepath = os.getcwd() + \"/video_chkpt/video_classifier\"\n",
    "transformer = get_transformer_model()\n",
    "transformer.load_weights(filepath)\n",
    "\n",
    "# evaluate the transformer model\n",
    "loss, accuracy, f1_score, precision, recall = transformer.evaluate(test_image_data, test_labels, verbose=0)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "print(f\"Precision: {round(precision, 2)}\")\n",
    "print(f\"Recall: {round(recall, 2)}\")\n",
    "\n",
    "# filepath = os.getcwd() + \"/temp/audio_classifier\"\n",
    "filepath = os.getcwd() + \"/audio_chkpt/audio_classifier\"\n",
    "cnn = get_cnn_model()\n",
    "cnn.load_weights(filepath)\n",
    "\n",
    "# evaluate the cnn model\n",
    "loss, accuracy, f1_score, precision, recall = cnn.evaluate(test_audio_data, test_labels, verbose=0)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "print(f\"Precision: {round(precision, 2)}\")\n",
    "print(f\"Recall: {round(recall, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_late_fusion():\n",
    "    ## Extract the probabilities from each classifier for the late fusion\n",
    "    res1 = transformer.predict(test_image_data)\n",
    "    # print(res1)\n",
    "    res2 = cnn.predict(test_audio_data)\n",
    "    # print(res2)\n",
    "    all_res = np.array([res1,res2])\n",
    "    all_res = all_res.sum(0)\n",
    "    return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Computing final prediction with late fusion without training\n",
    "# results1 = all_res.sum(0).argmax(1)\n",
    "# results2 = all_res.prod(0).argmax(1)\n",
    "# results3 = np.median(all_res, 0).argmax(1)\n",
    "# results4 = np.max(all_res, 0).argmax(1)\n",
    "\n",
    "# results = all_res.sum(0)\n",
    "\n",
    "\n",
    "def predictLabelForGivenThreshold(results, threshold):\n",
    "    # y_pred=[]\n",
    "    # for sample in results:\n",
    "    #     y_pred.append([1 if i>=threshold else 0 for i in sample ] )\n",
    "    # return np.array(y_pred)\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "    for key,values in enumerate(list(results)):\n",
    "        temp = []\n",
    "        for v in values:\n",
    "            v = (v >= threshold).astype(int)\n",
    "            temp.append(v)\n",
    "        predictions.append(temp) \n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Mature       0.24      1.00      0.38        24\n",
      "   Slapstick       0.42      0.28      0.33        18\n",
      "        Gory       0.30      0.50      0.37         6\n",
      "     Sarcasm       0.54      0.86      0.67        43\n",
      "\n",
      "   micro avg       0.36      0.76      0.49        91\n",
      "   macro avg       0.37      0.66      0.44        91\n",
      "weighted avg       0.42      0.76      0.51        91\n",
      " samples avg       0.36      0.55      0.42        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "label_names = ['Mature', 'Slapstick', 'Gory', 'Sarcasm']\n",
    "results = get_late_fusion()\n",
    "y_pred  = predictLabelForGivenThreshold(results,0.7)\n",
    "print(classification_report(test_labels, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 of each label: [0.38095238 0.33333333 0.375      0.66666667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"F1 of each label: {}\".format(metrics.f1_score(test_labels, y_pred, average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34112149532710273"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "Accuracy(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12149532710280374\n",
      "0.12149532710280374\n"
     ]
    }
   ],
   "source": [
    "#Exact match ratio\n",
    "MR = np.all(y_pred == test_labels, axis=1).mean()\n",
    "print(MR)\n",
    "print(accuracy_score(test_labels,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Mature       0.48      0.54      0.51        24\n",
      "   Slapstick       1.00      0.11      0.20        18\n",
      "        Gory       0.27      0.50      0.35         6\n",
      "     Sarcasm       0.72      0.72      0.72        43\n",
      "\n",
      "   micro avg       0.59      0.54      0.56        91\n",
      "   macro avg       0.62      0.47      0.45        91\n",
      "weighted avg       0.68      0.54      0.54        91\n",
      " samples avg       0.41      0.39      0.40        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\maitr\\AppData\\Local\\Temp\\ipykernel_1632\\920900653.py:4: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_res = transformer.predict(test_image_data)\n",
    "img_y_pred = predictLabelForGivenThreshold(img_res,0.6)\n",
    "print(classification_report(test_labels, img_y_pred,target_names=label_names))\n",
    "Accuracy(test_labels, img_y_pred)\n",
    "# print(img_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Mature       0.22      1.00      0.37        24\n",
      "   Slapstick       0.18      1.00      0.31        18\n",
      "        Gory       0.00      0.00      0.00         6\n",
      "     Sarcasm       0.40      1.00      0.57        43\n",
      "\n",
      "   micro avg       0.27      0.93      0.42        91\n",
      "   macro avg       0.20      0.75      0.31        91\n",
      "weighted avg       0.28      0.93      0.43        91\n",
      " samples avg       0.27      0.68      0.38        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26791277258566965"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_res = cnn.predict(test_audio_data)\n",
    "audio_y_pred = predictLabelForGivenThreshold(audio_res,0.1)\n",
    "print(classification_report(test_labels, audio_y_pred,target_names=label_names))\n",
    "Accuracy(test_labels, audio_y_pred)\n",
    "# print(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('extracted_test_spectrogram/al-TxOuSqc8.02.jpg')\n",
    "img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "img = np.array(img)\n",
    "# img = test_data[104]\n",
    "img = img.reshape((1,IMG_SIZE,IMG_SIZE,3))\n",
    "# print(img.shape)\n",
    "\n",
    "y_pred = cnn.predict(img)[0]\n",
    "\n",
    "# # round probabilities to class labels\n",
    "# y_pred = y_pred.round()\n",
    "\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9abb609d9af7c865ec2d4837dadf5771495602565493ad3e886cad7ec3618278"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf.2.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
