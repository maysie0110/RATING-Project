{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "37db3b7a-c81d-4f6a-b8a8-9f5674c6feb5",
    "deepnote_cell_type": "text-cell-h1",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# RATING Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4c574a17-beaa-467a-80e8-64c8c9578cbc",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Comic Mischief Detection Task\n",
    "\n",
    "Files:\n",
    "\n",
    "1. \"train.csv\" : \n",
    "-- Contains multiclass classification content annotations for each video scene used in the training set.\n",
    "-- Annotations are on a scene level and do not correspond to a specific modality\n",
    "-- a \".csv\" file containing video URLs as well as the IDs of the scenes used in the training set.\n",
    "-- Videos are available in the form of URLs, collected from the Youtube and the IMDB websites.\n",
    "-- Contains metadata about the videos.\n",
    "-- Four content categories related to comic mischief are used (Sarcasm, Slapstick Humor, Gory Humor, Mature Humor).\n",
    "\n",
    "2. \"val.csv\" : \n",
    "-- Contains multiclass classification content annotations for each video scene used in the validation set.\n",
    "-- You can use this set for performing model hyperparameter tuning before using the test set\n",
    "\n",
    "\n",
    "3. \"test.csv\" : \n",
    "-- Contains multiclass classification content annotations for each video scene used in the test set.\n",
    "-- You can use this set for evaluating your method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "ae028c11-f4b1-4180-9f60-74f25891fb1f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3894,
    "execution_start": 1644814279553,
    "source_hash": "94243ef8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score,accuracy_score, confusion_matrix\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_preprocessing import image\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_id": "09480eaa-48a0-439d-b679-23024487d866",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1644814284695,
    "source_hash": "2000a060",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## global variables\n",
    "\n",
    "train_dir = os.getcwd() + \"/train_data/\"\n",
    "val_dir = os.getcwd() + \"/val_data/\"\n",
    "test_dir = os.getcwd() + \"/test_data/\"\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_SEQ_LENGTH = 128\n",
    "FRAME_GAP = 11\n",
    "NUM_FEATURES = 1024\n",
    "IMG_SIZE = 224\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_data, train_labels = np.load(\"seq_length_128/extracted_data/train_data.npy\"), np.load(\"seq_length_128/extracted_data/train_labels.npy\")\n",
    "val_image_data, val_labels = np.load(\"seq_length_128/extracted_data/val_data.npy\"), np.load(\"seq_length_128/extracted_data/val_labels.npy\")\n",
    "test_image_data, test_labels = np.load(\"seq_length_128/extracted_data/test_data.npy\"), np.load(\"seq_length_128/extracted_data/test_labels.npy\")\n",
    "\n",
    "train_spectrograms = glob.glob('audio_classification/extracted_train_spectrogram/*')\n",
    "val_spectrograms = glob.glob('audio_classification/extracted_val_spectrogram/*')\n",
    "test_spectrograms = glob.glob('audio_classification/extracted_test_spectrogram/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to open video files using CV2\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    count = 0\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if count % 5 == 0:\n",
    "                frame = crop_center_square(frame)\n",
    "                frame = cv2.resize(frame, resize)\n",
    "                frame = frame[:, :, [2, 1, 0]]\n",
    "                frames.append(frame)\n",
    "\n",
    "            count=count+1\n",
    "            \n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Transformer-based model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "e36a7e79-836f-4942-a832-d53954293523",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "51248883",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "3e2e67d3-0540-476b-b812-04d019aa8269",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "521c40ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subclassed layer\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    # classes = len(label_processor.get_vocabulary())\n",
    "    classes = 4\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None))\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, None, None)]      0         \n",
      "_________________________________________________________________\n",
      "frame_position_embedding (Po (None, None, 1024)        131072    \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 1024)        4211716   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,346,888\n",
      "Trainable params: 4,346,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test accuracy: 54.21%\n",
      "F1 score: 0.59\n",
      "Precision: 0.54\n",
      "Recall: 0.69\n"
     ]
    }
   ],
   "source": [
    "filepath = os.getcwd() + \"/seq_length_128/video_chkpt_128/video_classifier\"\n",
    "transformer = get_transformer_model()\n",
    "transformer.load_weights(filepath)\n",
    "\n",
    "# evaluate the transformer model\n",
    "loss, accuracy, f1_score, precision, recall = transformer.evaluate(test_image_data, test_labels, verbose=0)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "print(f\"Precision: {round(precision, 2)}\")\n",
    "print(f\"Recall: {round(recall, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    # Pad shorter videos.\n",
    "    if len(frames) < MAX_SEQ_LENGTH:\n",
    "        diff = MAX_SEQ_LENGTH - len(frames)\n",
    "        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
    "        frames = np.concatenate((frames, padding))\n",
    "\n",
    "    frames = frames[None, ...]\n",
    "\n",
    "    # Extract features from the frames of the current video.\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            if np.mean(batch[j, :]) > 0.0:\n",
    "                frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "            else:\n",
    "                frame_features[i, j, :] = 0.0\n",
    "\n",
    "    return frame_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []\n",
    "actual = []\n",
    "\n",
    "for path in test_df['path']:\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features = prepare_single_video(frames)\n",
    "    y_pred = transformer_model.predict(frame_features)[0]\n",
    "    \n",
    "    # round probabilities to class labels\n",
    "    y_pred = y_pred.round()\n",
    "    predict.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.3516483516483517\n",
      "Precision 0.5134746206174778\n",
      "F1 score 0.4059426489003954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_true=test_labels, y_pred=predict, average='weighted')\n",
    "print(\"Recall\", recall)\n",
    "precision = precision_score(y_true=test_labels, y_pred=predict, average='weighted')\n",
    "print(\"Precision\", precision)\n",
    "f1 = f1_score(y_true=test_labels, y_pred=predict, average='weighted')\n",
    "print(\"F1 score\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.08      0.12        24\n",
      "           1       0.00      0.00      0.00        18\n",
      "           2       0.50      0.83      0.62         6\n",
      "           3       0.89      0.58      0.70        43\n",
      "\n",
      "   micro avg       0.68      0.35      0.46        91\n",
      "   macro avg       0.40      0.37      0.36        91\n",
      "weighted avg       0.51      0.35      0.41        91\n",
      " samples avg       0.27      0.27      0.27        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\maitr\\anaconda3\\envs\\tf.2.6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7327af42-8a03-4c46-b38e-e6931aa020f3' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CNN model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "\n",
    "    #Create CNN model\n",
    "    model_pretrained = VGG19(include_top=True, input_shape=(IMG_SIZE, IMG_SIZE, 3))#, weights=\"imagenet\")\n",
    "    x = layers.Dense(4096, activation='relu', name='predictions1', dtype='float32')(model_pretrained.layers[-2].output)\n",
    "    output = layers.Dense(4, activation='sigmoid', name='predictions', dtype='float32')(x)\n",
    "    model = Model(model_pretrained.input, output)\n",
    "\n",
    "\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.0000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # optimizer = keras.optimizers.SGD(learning_rate=0.0000001, decay=1e-6, momentum=0.9)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_data = []\n",
    "val_audio_data = []\n",
    "test_audio_data = []\n",
    "\n",
    "for f in train_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    train_audio_data.append(img)\n",
    "    \n",
    "train_audio_data = np.array(train_audio_data)\n",
    "\n",
    "for f in val_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    val_audio_data.append(img)\n",
    "    \n",
    "val_audio_data = np.array(val_audio_data)\n",
    "\n",
    "for f in test_spectrograms:\n",
    "    img = image.load_img(f, target_size= (IMG_SIZE,IMG_SIZE))\n",
    "    img = image.img_to_array(img)\n",
    "    test_audio_data.append(img)\n",
    "    \n",
    "test_audio_data = np.array(test_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions1 (Dense)         (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 156,367,940\n",
      "Trainable params: 156,367,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Test accuracy: 30.84%\n",
      "F1 score: 0.41\n",
      "Precision: 0.32\n",
      "Recall: 0.6\n"
     ]
    }
   ],
   "source": [
    "filepath = os.getcwd() + \"/audio_classification/audio_chkpt_sgd/audio_classifier\"\n",
    "# filepath = os.getcwd() + \"/audio_temp/audio_classifier\"\n",
    "cnn_model = get_cnn_model()\n",
    "cnn_model.load_weights(filepath)\n",
    "\n",
    "# evaluate the cnn model\n",
    "loss, accuracy, f1_score, precision, recall = cnn_model.evaluate(test_audio_data, test_labels, verbose=0)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"F1 score: {round(f1_score, 2)}\")\n",
    "print(f\"Precision: {round(precision, 2)}\")\n",
    "print(f\"Recall: {round(recall, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn_model.predict(test_audio_data)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "# print(y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, y_pred))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_action(path):\n",
    "#     class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "#     frames = load_video(os.path.join(\"test\", path))\n",
    "#     frame_features = prepare_single_video(frames)\n",
    "#     probabilities = trained_model.predict(frame_features)[0]\n",
    "\n",
    "#     for i in np.argsort(probabilities)[::-1]:\n",
    "#         print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "\n",
    "#     # pred = np.argsort(probabilities)[::-1]\n",
    "#     # print(pred)\n",
    "#     return frames\n",
    "\n",
    "\n",
    "# # This utility is for visualization.\n",
    "# # Referenced from:\n",
    "# # https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "# def to_gif(images):\n",
    "#     converted_images = images.astype(np.uint8)\n",
    "#     imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
    "#     return embed.embed_file(\"animation.gif\")"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "05ab0b75-b65c-4d2b-b56a-635b53176e13",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
